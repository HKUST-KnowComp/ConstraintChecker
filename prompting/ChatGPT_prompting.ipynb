{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDD0MSRckqCL"
      },
      "source": [
        "## Setting + Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjwwwjO9izyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859d4dd1-b3d2-4d48-821c-884bcee68e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai\n",
        "import openai, time\n",
        "import pandas as pd\n",
        "\n",
        "!rm -f data\n",
        "!ln -s '<path_to_data_folder>' data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-LasnAI6Rgd"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"\"\n",
        "predictions = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt(prompt, max_tokens=5):\n",
        "    while True:\n",
        "        try:\n",
        "            return openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo-0301\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=max_tokens, temperature=0\n",
        "            )['choices'][0]['message']['content'].strip()\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n",
        "\n",
        "def davinci(prompt, max_tokens=5):\n",
        "    while True:\n",
        "        try:\n",
        "            return openai.Completion.create(\n",
        "                model=\"gpt-3.5-turbo-instruct\", #\"text-davinci-003\",\n",
        "                prompt=[prompt],\n",
        "                max_tokens=max_tokens, temperature=0\n",
        "            )['choices'][0]['text'].strip()\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "bTcKaqIul55u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B89YstSmlkGO"
      },
      "outputs": [],
      "source": [
        "# Tq's modified implementation\n",
        "import asyncio\n",
        "import time\n",
        "\n",
        "async def dispatch_openai_requests(\n",
        "    messages_list,\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    max_tokens=4,\n",
        "    ):\n",
        "    \"\"\"Dispatches requests to OpenAI API asynchronously.\"\"\"\n",
        "    async_responses = [\n",
        "        openai.ChatCompletion.acreate(\n",
        "            model=model,\n",
        "            messages=x,\n",
        "            temperature=0,\n",
        "            max_tokens=max_tokens,\n",
        "            top_p=1,\n",
        "        )\n",
        "        for x in messages_list\n",
        "    ]\n",
        "    time.sleep(1)\n",
        "    return await asyncio.gather(*async_responses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw1LNjFLxMVp"
      },
      "outputs": [],
      "source": [
        "data_df = pd.read_csv('data/dev_data_to_tune.csv')\n",
        "data = data_df['assertion'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUaXbMySuzXH",
        "outputId": "16b1d03e-7ad3-41ce-81ed-d1babc28e4c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172.28.0.12 \n"
          ]
        }
      ],
      "source": [
        "!hostname -I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ6uzg86kt1k"
      },
      "source": [
        "## Neubig's code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKHsHOQDjsRY"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from typing import Any\n",
        "\n",
        "async def dispatch_openai_requests(\n",
        "    messages_list: list[list[dict[str,Any]]],\n",
        "    model: str,\n",
        "    temperature: float,\n",
        "    max_tokens: int,\n",
        "    top_p: float,\n",
        ") -> list[str]:\n",
        "    \"\"\"Dispatches requests to OpenAI API asynchronously.\n",
        "\n",
        "    Args:\n",
        "        messages_list: List of messages to be sent to OpenAI ChatCompletion API.\n",
        "        model: OpenAI model to use.\n",
        "        temperature: Temperature to use for the model.\n",
        "        max_tokens: Maximum number of tokens to generate.\n",
        "        top_p: Top p to use for the model.\n",
        "\n",
        "    Returns:\n",
        "        List of responses from OpenAI API.\n",
        "    \"\"\"\n",
        "    async_responses = [\n",
        "        openai.ChatCompletion.acreate(\n",
        "            model=model,\n",
        "            messages=x,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            top_p=top_p,\n",
        "        )\n",
        "        for x in messages_list\n",
        "    ]\n",
        "    return await asyncio.gather(*async_responses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ky4h1EQkCoA"
      },
      "outputs": [],
      "source": [
        "predictions = asyncio.run(\n",
        "    dispatch_openai_requests(\n",
        "        messages_list=[\n",
        "            [{\"role\": \"user\", \"content\": \"Write a poem about asynchronous execution.\"}],\n",
        "            [{\"role\": \"user\", \"content\": \"Write a poem about asynchronous pirates.\"}],\n",
        "        ],\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        temperature=0.3,\n",
        "        max_tokens=200,\n",
        "        top_p=1.0,\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSjmMAZylhQ6"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fncOzerakDqK"
      },
      "source": [
        "### Direct judge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tc5BFYCb1rU"
      },
      "source": [
        "A hypothesis\n",
        "- the longer prompt is, the worse result we get"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFRoNTBc675M"
      },
      "outputs": [],
      "source": [
        "judge_prompt_0 = \"Answer whether the following statement is plausible. Answer with only Yes or No:\\n{}\"\n",
        "judge_prompt_1 = \"Judge the following statement if it's likely to occur, only answer 'True' or 'False':\\n{}\"\n",
        "judge_prompt_1_different_question = \"Please judge if the following inference is likely 'True' or 'False'. Only answer 'True' or 'False':\\n{}\"\n",
        "judge_prompt_4 = \"Judge the following statements if they are likely to occur, only answer 'True' or 'False' for each:\\n{}\"\n",
        "judge_prompt_2_cms = \"Judge the following statement if it's commonsense, only answer 'True' or 'False':\\n{}\"\n",
        "judge_prompt_3_guidance = \"\"\"Your task is to judge a statement/inference if it's likely to occur. Only answer 'True' or 'False'.\n",
        "Please not only consider the relevance of two clauses in the statements/inferences, but also the aspect that it refers to, e.g emotion, intention, attribute, etc.\n",
        "For example,\n",
        "## Example 1\n",
        "Statement: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent\n",
        "Answer: True\n",
        "\n",
        "## Example 2\n",
        "Statement: PersonX be smart, thus it can be seen about PersonX's attribute that PersonX have always make good grade\n",
        "Answer: False\n",
        "\n",
        "## Example 3\n",
        "Statement: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "Answer: False\n",
        "\n",
        "Now, please do your task for each of the following statement(s):\n",
        "{}\"\"\"\n",
        "judge_prompt_5_guidance = \"Judge the below statement/inference if it's likely to occur, only answer 'True' or 'False'. Please not only consider the relevance of two clauses in the statements/inferences, but also the aspect that it refers to, e.g emotion, intention, attribute, etc.\\n{}\"\n",
        "judge_prompt_6_guidance = \"\"\"Your task is to do the following step by step, given the original 'Statement'.\n",
        "Thinking: Think about the reason if it's likely to occur\n",
        "Answer: Judge if the statement is likely to occur. Only answer 'True' or 'False'.\n",
        "\n",
        "For example,\n",
        "## Example 1: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent\n",
        "Thinking: People tend to applaud other people's effort and talent\n",
        "Answer: True\n",
        "\n",
        "## Example 2: PersonX be smart, thus it can be seen about PersonX's attribute that PersonX have a good grade\n",
        "Thinking: Though \"PersonX have a good grade\" tends to happen when PersonX is smart, it is a fact rather than an attribute of PersonX\n",
        "Answer: False\n",
        "\n",
        "## Example 3: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "Thinking: \"only make it through half\" is not related to PersonX's emotion or feeling\n",
        "Answer: False\n",
        "\n",
        "Now, please do your task for each of the following statement(s):\n",
        "{}\"\"\"\n",
        "judge_prompt_7 = \"\"\"Judge the following statement(s) if it's likely to occur, answer 'True' or 'False' then explain briefly:\n",
        "Example: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "Answer: False. \"only make it through half\" is not related to PersonX's emotion or feeling\n",
        "\n",
        "Example: {}\n",
        "Answer: \"\"\"\n",
        "\n",
        "conceptualization_prompt = \"\"\"Your task is to do the following step by step, given the original statement.\n",
        "1. find a phrase in the statement (phrases in the last clause of the statement are preferable)\n",
        "2. conceptualize it in a concise manner\n",
        "3. write down the conceptualized statement formed by direct substitution\n",
        "4. based on both original and conceptualized statements, judge if the original statement likely to occur by answering 'True' or 'False'. After that, you may briefly explain within one sentence.\n",
        "\n",
        "For example,\n",
        "\n",
        "## Example 1\n",
        "Statement: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent\n",
        "1. the trumpet\n",
        "2. instrument\n",
        "3. PersonX learns to play instrument, thus PersonY wants to applaud PersonX's talent\n",
        "4. True. Because people tend to applaud other people's effort and talent.\n",
        "\n",
        "## Example 2\n",
        "Statement: PersonX wins the costume contest, thus, PersonX feel excited\n",
        "1. the costume contest\n",
        "2. competition\n",
        "3. PersonX wins competition, thus PersonX feel excited\n",
        "4. True. Because it's very happy to win a competition.\n",
        "\n",
        "## Example 3\n",
        "Statement: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "1. only make it through half\n",
        "2. don't finish\n",
        "3. PersonX do not feel well, thus as a result on PersonX's emotion, PersonX don't finish\n",
        "4. False. Because \"don't finish\" is an action, rather than an emotion.\n",
        "\n",
        "Now, please do your task for each of the following statement(s):\n",
        "Statement: {}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W_2sIT_hug2"
      },
      "source": [
        "#### Template 0: ckbp v2 paper\n",
        "\n",
        "worse than template 1 in both dev and tst. But seem to be less strict (as True/False seems to be more strict than Yes/No), thus try on text-davinci-003\n",
        "\n",
        "Hmm, it's better, but far from gpt3.5-turbo_template1\n",
        "\n",
        "Actually, by running these two prompts, we can see how sensitive LLMs are w.r.t different prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxKbgG6qhzuu"
      },
      "outputs": [],
      "source": [
        "bs = 102\n",
        "predictions = []\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(data)//bs):\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": judge_prompt_0.format(s)}] for s in data[i*bs:(i+1)*bs]],\n",
        "            max_tokens=3,\n",
        "        )\n",
        "    print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "\n",
        "end = time.time()\n",
        "print('Running time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2-j0t-DCocn"
      },
      "source": [
        "#### Template 1: judge_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb0rUpfeoO6z"
      },
      "outputs": [],
      "source": [
        "bs = 102\n",
        "predictions = []\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(data)//bs):\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": judge_prompt_1.format(s)}] for s in data[i*bs:(i+1)*bs]],\n",
        "            max_tokens=3,\n",
        "        )\n",
        "    print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "    time.sleep(10)\n",
        "\n",
        "end = time.time()\n",
        "print('Running time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxkNW7Eou23G"
      },
      "outputs": [],
      "source": [
        "fail_indices = [i for i, a in enumerate(predictions) if ('True' not in a) and ('False' not in a)]\n",
        "if len(fail_indices) > 0:\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": judge_prompt_1.format(data[i])}] for i in fail_indices],\n",
        "            max_tokens=3,\n",
        "        )\n",
        "    print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    for i, x in zip(fail_indices, pred):\n",
        "        predictions[i] = x['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yvoEzc6Llhf"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    print(data[i])\n",
        "    pred = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": judge_prompt_1.format(data[i])}],\n",
        "        max_tokens=4, temperature=0\n",
        "    )\n",
        "    print(pred['choices'][0]['message']['content'])\n",
        "    predictions.append(pred['choices'][0]['message']['content'])\n",
        "    time.sleep(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8Nvb6EdhZLQ"
      },
      "outputs": [],
      "source": [
        "bs = 3\n",
        "for i in range(len(data)//bs+1):\n",
        "    prompt = judge_prompt_1.format('\\n'.join(data[bs*i:bs*(i+1)]))\n",
        "    gen_chatgpt = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        # max_tokens=2000,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(gen_chatgpt['choices'][0]['message']['content'])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW56K34kCjyX"
      },
      "source": [
        "#### Template 2: judge_prompt_cms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUs4hiCICdKg"
      },
      "outputs": [],
      "source": [
        "bs = 102\n",
        "predictions = []\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(data)//bs):\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": judge_prompt_2_cms.format(s)}] for s in data[i*bs:(i+1)*bs]],\n",
        "            max_tokens=3,\n",
        "        )\n",
        "    print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "    time.sleep(10)\n",
        "\n",
        "end = time.time()\n",
        "print('Running time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBh47_Cw7aJc"
      },
      "source": [
        "#### Template 3 with guidance judge_prompt_guidance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMBTeS3w7ek1"
      },
      "outputs": [],
      "source": [
        "bs = 20\n",
        "for i in range(len(data)//bs+1):\n",
        "    prompt = judge_prompt_3_guidance.format('\\n'.join(data[bs*i:bs*(i+1)]))\n",
        "    gen_chatgpt = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        # max_tokens=2000,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(gen_chatgpt['choices'][0]['message']['content'])\n",
        "    time.sleep(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaWpP8BgEORs"
      },
      "source": [
        "Template 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1DjRk0iEP-1"
      },
      "outputs": [],
      "source": [
        "bs = 18\n",
        "for i in range(len(data)//bs+1):\n",
        "    prompt = judge_prompt_4.format('\\n'.join(data[bs*i:bs*(i+1)]))\n",
        "    gen_chatgpt = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        # max_tokens=2000,\n",
        "        temperature=0\n",
        "    )\n",
        "    print('\\n', gen_chatgpt['choices'][0]['message']['content'])\n",
        "    time.sleep(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZs5XoaOHGBS"
      },
      "outputs": [],
      "source": [
        "bs = 1\n",
        "for i in range((len(data)-1)//bs+1):\n",
        "    prompt = judge_prompt_4.format('\\n'.join(data[bs*i:bs*(i+1)]))\n",
        "    gen_chatgpt = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        # max_tokens=2000,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(gen_chatgpt['choices'][0]['message']['content'])\n",
        "    time.sleep(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8aXR5tTV_Cv"
      },
      "source": [
        "Template 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSwnuz2WV1dH"
      },
      "outputs": [],
      "source": [
        "bs = 102\n",
        "predictions = []\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(data)//bs):\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": judge_prompt_5_guidance.format(s)}] for s in data[i*bs:(i+1)*bs]],\n",
        "            max_tokens=3,\n",
        "        )\n",
        "    print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "    time.sleep(10)\n",
        "\n",
        "end = time.time()\n",
        "print('Running time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqKy-nppipR4"
      },
      "source": [
        "Template 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq9H_oXkbVY1"
      },
      "outputs": [],
      "source": [
        "bs = 102\n",
        "predictions = []\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(data)//bs):\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": judge_prompt_6_guidance.format(s)}] for s in data[i*bs:(i+1)*bs]],\n",
        "            max_tokens=100,\n",
        "        )\n",
        "    print('\\n\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "    time.sleep(10)\n",
        "\n",
        "end = time.time()\n",
        "print('Running time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hTbfjXkjoE6"
      },
      "source": [
        "Template 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRmfr2P2jsM7"
      },
      "outputs": [],
      "source": [
        "bs = 102\n",
        "predictions = []\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(data)//bs):\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": judge_prompt_7.format(s)}] for s in data[i*bs:(i+1)*bs]],\n",
        "            max_tokens=30,\n",
        "        )\n",
        "    print('\\n\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "    time.sleep(10)\n",
        "\n",
        "end = time.time()\n",
        "print('Running time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlyqsANuyIPb"
      },
      "source": [
        "### Zeroshot CoT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zeroshotCoT_prompt_tq = \"Answer whether the statement '{}' is plausible. Let's think step by step, then conclude by answering 'True' or 'False'.\"\n",
        "zeroshotCoT_prompt1 = \"Judge the statement '{}' if it's likely to occur. Let's think step by step, then conclude by answering 'True' or 'False'.\" # if add only, it will strictly follow that command, only produce 'True' or 'False' w/o CoT\n",
        "# the combination zeroshotCoT_prompt_tq + assertion_tq is so baddd!\n",
        "# naming rule:\n",
        "# zeroshot_prompt1 = zeroshotCoT_prompt_tq + assertion\n",
        "# zeroshot_prompt2 = zeroshotCoT_prompt1 + assertion_tq\n",
        "# zeroshot_prompt3 = zeroshotCoT_prompt1 + assertion"
      ],
      "metadata": {
        "id": "KIrDq447WJH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt_tq.format(data_df['assertion'][0])}],\n",
        "        max_tokens=100, temperature=0\n",
        "    )['choices'][0]['message']['content'],\n",
        "    openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt_tq.format(data_df['assertion'][4])}],\n",
        "        max_tokens=100, temperature=0\n",
        "    )['choices'][0]['message']['content'],\n",
        "    openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt1.format(data_df['assertion_tq'][0])}],\n",
        "        max_tokens=100, temperature=0\n",
        "    )['choices'][0]['message']['content'],\n",
        "    openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt1.format(data_df['assertion_tq'][4])}],\n",
        "        max_tokens=100, temperature=0\n",
        "    )['choices'][0]['message']['content'],\n",
        "    sep='\\n'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CibsTYCecf4",
        "outputId": "f89f8c3b-83e5-4515-bc7b-120bb1cf596b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True.\n",
            "True.\n",
            "Step 1: Ejaculation is a physical response that occurs when a person reaches sexual climax.\n",
            "Step 2: Wanting to ejaculate is a psychological desire that may or may not be present during sexual activity.\n",
            "Step 3: Orgasm is the peak of sexual pleasure and can occur with or without ejaculation.\n",
            "Step 4: Therefore, it is possible for a person to ejaculate without wanting to or without experiencing an orgasm.\n",
            "\n",
            "Conclusion: False. Ejaculation does not necessarily indicate a\n",
            "Step 1: If PersonX enlist\n",
            "Step 2: As a result, PersonX wants to\n",
            "Step 3: PersonX wants to fight for country\n",
            "\n",
            "Conclusion: It is likely to occur that if PersonX enlist, as a result, PersonX wants to fight for country. True.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt_tq.format(data_df['assertion'][0])}],\n",
        "        max_tokens=100, temperature=0\n",
        "    )['choices'][0]['message']['content'],\n",
        "    openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt_tq.format(data_df['assertion'][4])}],\n",
        "        max_tokens=100, temperature=0\n",
        "    )['choices'][0]['message']['content'],\n",
        "    openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt_tq.format(data_df['assertion_tq'][0])}],\n",
        "        max_tokens=100, temperature=0\n",
        "    )['choices'][0]['message']['content'],\n",
        "    openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt_tq.format(data_df['assertion_tq'][4])}],\n",
        "        max_tokens=100, temperature=0\n",
        "    )['choices'][0]['message']['content'],\n",
        "    sep='\\n'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pp-dbgThlcJ",
        "outputId": "86e8bd75-6451-466e-a15d-7988b7a3a925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True.\n",
            "True.\n",
            "As an AI language model, I cannot determine the plausibility of the statement as it is incomplete and lacks context. Please provide more information or rephrase the statement.\n",
            "As an AI language model, I cannot determine the intention or desire of a person. Therefore, I cannot determine the plausibility of the statement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt1.format(data_df['assertion'][0])}],\n",
        "        max_tokens=100, temperature=0\n",
        "    )['choices'][0]['message']['content'],\n",
        "    openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt1.format(data_df['assertion'][4])}],\n",
        "        max_tokens=100, temperature=0\n",
        "    )['choices'][0]['message']['content'],\n",
        "    sep='\\n'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE7yTy4ufvUS",
        "outputId": "de400e27-4900-4a04-8bc8-3a7d188285e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Ejaculation does not always indicate that a person wants an orgasm. It is possible for a person to ejaculate without wanting an orgasm, such as during a medical procedure or due to a reflex action.\n",
            "\n",
            "Step 2: However, in most cases, ejaculation is a result of sexual stimulation and is often accompanied by the desire for an orgasm.\n",
            "\n",
            "Step 3: Therefore, it is likely that if PersonX ejaculates, PersonX wants an orgasm.\n",
            "\n",
            "Conclusion: True.\n",
            "Step 1: Enlisting in the military is a common way to show a desire to serve one's country.\n",
            "True.\n",
            "\n",
            "Step 2: However, there may be other reasons why PersonX enlisted, such as financial benefits or a desire for adventure.\n",
            "True.\n",
            "\n",
            "Step 3: Without knowing PersonX's specific motivations for enlisting, it is impossible to definitively say whether or not they want to fight for their country.\n",
            "True.\n",
            "\n",
            "Conclusion: The statement \"PersonX enlisted, thus, Person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    messages=[{\"role\": \"user\", \"content\": zeroshotCoT_prompt_tq.format(data_df['assertion'][0])}],\n",
        "    max_tokens=100, temperature=0\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bafQ5NuSeD-8",
        "outputId": "7eca64a8-75a2-4b45-ae4e-850e04b6bd85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7LRswM6EXKbnVkz2dRUYzNGxOWgxB at 0x7f4f1ea787c0> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"As an AI language model, I cannot determine the plausibility of a statement without additional context. However, in general, ejaculation does not necessarily indicate that a person wants an orgasm. Ejaculation can occur without orgasm, and some people may choose to ejaculate without seeking orgasm. It ultimately depends on the individual's preferences and desires.\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1685346434,\n",
              "  \"id\": \"chatcmpl-7LRswM6EXKbnVkz2dRUYzNGxOWgxB\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 68,\n",
              "    \"prompt_tokens\": 37,\n",
              "    \"total_tokens\": 105\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Answer whether the statement '{}' is plausible.\".format(data_df['assertion_tq'][0])}],\n",
        "    max_tokens=100, temperature=0\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi1bidA8dfGo",
        "outputId": "65d2f48d-5346-4511-a826-d52e3614f3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7LRpiOQR0aL7YC1cVyCwNRaDSFb1R at 0x7f4f1ea57e20> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"As an AI language model, I cannot determine the plausibility of the statement as it is incomplete and lacks context. Please provide more information or rephrase the statement for me to provide a proper answer.\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1685346234,\n",
              "  \"id\": \"chatcmpl-7LRpiOQR0aL7YC1cVyCwNRaDSFb1R\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 41,\n",
              "    \"prompt_tokens\": 35,\n",
              "    \"total_tokens\": 76\n",
              "  }\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWDPz-lqbMju"
      },
      "source": [
        "Try several CoT prompts below, and several ways to place the instruction. Two main problems\n",
        "- they are not abstraction\n",
        "- they don't (or me don't know how to) produce a fixed answer term, when run in batch, don't know how to judge automatically!\n",
        "\n",
        "Also, don't know if using role message is better than normal prompt for gpt-3.5-turbo, not run all dev instance yet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efPZg4C4U_wC"
      },
      "outputs": [],
      "source": [
        "role = \"You are a commonsense knowledge judge, who gives final answer as 'Yes' or 'No'. Given a statement without any additional context, \"+\\\n",
        "    \"please focus on the last clause in the statement and judge if it's likely to occur given the context before it.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUt6x_ZRyW59"
      },
      "outputs": [],
      "source": [
        "cot_prompt_1 = \"Judge the following statement if it's likely to occur:\\n{}\"\n",
        "role = \"You are a commonsense knowledge judge. Given a statement without any additional context, \"+\\\n",
        "    \"please focus on the last clause in the statement and judge if it's likely to occur given the context before it.\"\n",
        "cot_prompt_2 = \"Please focus on the last clause in the statement and judge if it's likely to occur given the context before it.\\n{}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iddARL1yJU2"
      },
      "outputs": [],
      "source": [
        "for i in [34]:\n",
        "    print(data[i])\n",
        "    pred = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[\n",
        "            # {\"role\": \"system\", \"content\": role},\n",
        "            {\"role\": \"user\", \"content\": cot_prompt_2.format(data[i])} # + \"\\n\\nLet's think step by step\"},\n",
        "        ],\n",
        "        # max_tokens=4,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(pred['choices'][0]['message']['content'])\n",
        "    # predictions.append(pred['choices'][0]['message']['content'])\n",
        "    # time.sleep(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VEr30WzXvtb"
      },
      "outputs": [],
      "source": [
        "for i in [34]:\n",
        "        messages=[\n",
        "            # {\"role\": \"system\", \"content\": role},\n",
        "            {\"role\": \"user\", \"content\": cot_prompt_2.format(data[i])}\n",
        "        ],\n",
        "\n",
        "# Given ..., likely ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hV2boFncXq0i"
      },
      "outputs": [],
      "source": [
        "for i in [34]:\n",
        "        messages=[\n",
        "            # {\"role\": \"system\", \"content\": role},\n",
        "            {\"role\": \"user\", \"content\": cot_prompt_2.format(data[i])}\n",
        "        ],\n",
        "\n",
        "# As an AI language model, I cannot determine the likelihood of an event occurring without additional context. Please provide more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kCEOaFaXBGi"
      },
      "outputs": [],
      "source": [
        "cot_prompt_2 = \"Please focus on the last clause in the statement and judge if it's likely to occur given the context before it.\\n{}\"\n",
        "\n",
        "for i in [38]:\n",
        "        messages=[\n",
        "            # {\"role\": \"system\", \"content\": role},\n",
        "            {\"role\": \"user\", \"content\": cot_prompt_2.format(data[i])}\n",
        "        ],"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQnX4rCRUE0D"
      },
      "outputs": [],
      "source": [
        "for i in [38]:\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": role},\n",
        "            {\"role\": \"user\", \"content\": data[i] + \"\\n\\nLet's think step by step\"},\n",
        "        ],"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiW9Opr5S5_V"
      },
      "outputs": [],
      "source": [
        "role = \"You are a commonsense knowledge judge. Given a statement without any additional context, \"+\\\n",
        "    \"please focus on the last clause in the statement and judge if it's likely to occur given the context before it. \"+\\\n",
        "    \"You may think step by step, but at the end, please conclude your answer in short as 'True' or 'False'.\"\n",
        "\n",
        "for i in [34]:\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": role},\n",
        "            {\"role\": \"user\", \"content\": data[i] + \"\\n\\nLet's think step by step\"},\n",
        "        ],"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMPk9lVUR0r8"
      },
      "outputs": [],
      "source": [
        "role = \"You are a commonsense knowledge judge. Given a statement without any additional context, \"+\\\n",
        "    \"please focus on the last clause in the statement and judge if it's likely to occur given the context before it. \"\n",
        "\n",
        "for i in [34]:\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": role},\n",
        "            {\"role\": \"user\", \"content\": data[i] + \"\\n\\nLet's think step by step\"},\n",
        "        ],"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyQVnVgeROVr"
      },
      "outputs": [],
      "source": [
        "for i in [38]:\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": role},\n",
        "            {\"role\": \"user\", \"content\": data[i] + \"\\n\\nLet's think step by step\"},\n",
        "        ],"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulY748jI6OQt"
      },
      "outputs": [],
      "source": [
        "messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a commonsense knowledge judge\"},\n",
        "            {\"role\": \"user\", \"content\": cot_prompt_1.format(data[i]) + \"\\nLet's think step by step\"},\n",
        "        ],"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfapkK-N1HN0"
      },
      "outputs": [],
      "source": [
        "messages=[\n",
        "            {\"role\": \"user\", \"content\": cot_prompt_1.format(data[i]) + \"\\nLet's think step by step\"}\n",
        "        ],\n",
        "\n",
        "# OMG! the answer is totally different, by just changing the position of the term \"let's think step by step\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAV7HN-B0hgy"
      },
      "outputs": [],
      "source": [
        "messages=[\n",
        "            {\"role\": \"user\", \"content\": cot_prompt_1.format(data[i])},\n",
        "            {\"role\": \"assistant\", \"content\": \"Let's think step by step\"}\n",
        "        ],"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOMyEs9DxjHP"
      },
      "source": [
        "### Abstraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7Dd_kH1a1u1"
      },
      "source": [
        "**Why not** abstract the tail into the aspect it expresses, then match with relation?\n",
        "\n",
        "Will prove that Automated-CoT or normal CoT will not solve CKBP!\n",
        "While for other commonsense datasets CSQA/StrategyQA, our method is better (of course, human effort is better)\n",
        "\n",
        "Very unpredictable! more exemplar, worse judgment! (see text-davinci-003 draft experiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjP1_Sqyc4Lu"
      },
      "outputs": [],
      "source": [
        "old_conceptualization_prompt = \"\"\"Your task is to do the following step by step, given the original statement.\n",
        "1. find a phrase in the statement (phrases in the last clause of the statement are preferable)\n",
        "2. conceptualize it in a concise manner\n",
        "3. write down the conceptualized statement formed by direct substitution\n",
        "4. based on both original and conceptualized statements, judge if the original statement likely to occur by answering 'True' or 'False'. After that, you may briefly explain within one sentence.\n",
        "\n",
        "For example,\n",
        "\n",
        "## Example 1\n",
        "Statement: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent\n",
        "1. the trumpet\n",
        "2. instrument\n",
        "3. PersonX learns to play instrument, thus PersonY wants to applaud PersonX's talent\n",
        "4. True. Because people tend to applaud other people's effort and talent.\n",
        "\n",
        "## Example 2\n",
        "Statement: PersonX wins the costume contest, thus, PersonX feel excited\n",
        "1. the costume contest\n",
        "2. competition\n",
        "3. PersonX wins competition, thus PersonX feel excited\n",
        "4. True. Because it's very happy to win a competition.\n",
        "\n",
        "## Example 3\n",
        "Statement: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "1. only make it through half\n",
        "2. don't finish\n",
        "3. PersonX do not feel well, thus as a result on PersonX's emotion, PersonX don't finish\n",
        "4. False. Because \"don't finish\" is an action, rather than an emotion.\n",
        "\n",
        "Now, please do your task for each of the following statement(s):\n",
        "{}\"\"\"\n",
        "\n",
        "conceptualization_prompt_5exemplars = \"\"\"Please read the convention, task description, examples, then do the task:\n",
        "Convention: There are three dimensions ['persona', 'event', 'mental state']. All ordered dimension pairs are valid, except ('event', 'persona') and ('event', 'mental state') are invalid.\n",
        "\n",
        "Task description: Given an inference and its corresponding dimension D_i and its two main clauses H and T, you are required to do the following step by step:\n",
        "1. Focus on the last clause T, determine which dimension best describe what the clause expresses. Denote the dimension as D_c.\n",
        "2. Judge if the pair (D_c, D_i) is valid. Answer 'Yes' or 'No'.\n",
        "3. Judge if the main clauses' meaning is all clear/non-ambiguous. Answer 'Yes' or 'No'.\n",
        "4. Based on result in 2, 3, and the semantics of the inference, judge if the inference is likely 'True' or 'False'. Note that if result of either 2 or 3 is 'No', then result of 4 should be 'False'.\n",
        "\n",
        "Examples:\n",
        "* Example 1\n",
        "Inference: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent\n",
        "D_i = 'event', H = 'PersonX learns to play the trumpet', T = 'PersonY wants to applaud PersonX's talent'\n",
        "1. D_c = 'event'\n",
        "2. Yes (because the ordered pair (D_c, D_i) = ('event', 'event') is valid)\n",
        "3. Yes\n",
        "4. True. Because 1) \"PersonX learns to play the trumpet\" means PersonX are talent and effortful in learning musical instrument, and 2) people tend to applaud other people's effort and talent.\n",
        "\n",
        "* Example 2\n",
        "Inference: PersonX eat most of those, thus it can be seen about PersonX's intention that, PersonX love curly fries\n",
        "D_i = 'event', H = 'PersonX eat most of those', T = 'PersonX love curly fries'\n",
        "1. D_c = 'mental state'\n",
        "2. Yes (because the ordered pair (D_c, D_i) = ('mental state', 'event') is valid)\n",
        "3. Yes\n",
        "4. False. Because 'love curly fries' expresses PersonX's preference rather than intention.\n",
        "\n",
        "* Example 3\n",
        "Inference: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "D_i = 'mental state', H = 'PersonX do not feel well', T = 'PersonX only make it through half'\n",
        "1. D_c = 'event'\n",
        "2. No (because the ordered pair (D_c, D_i) = ('event', 'mental state') is not valid)\n",
        "3. Yes\n",
        "4. False. Because, result of 2 is No.\n",
        "\n",
        "* Example 4\n",
        "Inference: Before PersonX really empathize with PersonY, PersonX feel.\n",
        "D_i = 'event', H = 'PersonX really empathize with PersonY', T = 'PersonX feel'\n",
        "1. D_c = 'mental state'\n",
        "2. Yes (because the ordered pair (D_c, D_i) = ('mental state', 'event') is valid)\n",
        "3. No (T is unclear about what 'PersonX feel')\n",
        "4. False. Because result of 3 is No.\n",
        "\n",
        "* Example 5\n",
        "Inference: PersonX have a traumatic incident, thus it can be seen about PersonX's attribute that PersonX be introvert\n",
        "D_i = 'persona', H = 'PersonX have a traumatic incident', T = 'PersonX be introvert'\n",
        "1. D_c = 'persona'\n",
        "2. Yes (because the ordered pair (D_c, D_i) = ('persona', 'persona') is valid)\n",
        "3. Yes\n",
        "4. False. Because having a traumatic incident likely result from unfortune or carelessness, while being introvert is not relevant.\n",
        "\n",
        "Now, please do your task for (each of) the following statement(s):\n",
        "Inference: {}\n",
        "D_i = '{}', H = '{}', T = '{}'\"\"\"\n",
        "\n",
        "\n",
        "conceptualization_prompt = \"\"\"Please read the convention, task description, examples, then do the task:\n",
        "Convention: There are three dimensions ['persona', 'event', 'mental state']. All ordered dimension pairs are valid, except ('event', 'persona') and ('event', 'mental state') are invalid.\n",
        "\n",
        "Task description: Given an inference and its corresponding dimension D_i and its two main clauses H and T, you are required to do the following step by step:\n",
        "1. Focus on the last clause T, determine which dimension best describe what the clause expresses. Denote the dimension as D_c.\n",
        "2. Judge if the ordered pair (D_c, D_i) is valid. Answer 'Yes' or 'No'.\n",
        "3. Judge if the main clauses' meaning is all clear/non-ambiguous. Answer 'Yes' or 'No'.\n",
        "4. Based on result in 2, 3, and the semantics of the inference, judge if the inference is likely 'True' or 'False'. Note that if result of either 2 or 3 is 'No', then result of 4 should be 'False'.\n",
        "\n",
        "Examples:\n",
        "* Example 1\n",
        "Inference: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent\n",
        "D_i = 'event', H = 'PersonX learns to play the trumpet', T = 'PersonY wants to applaud PersonX's talent'\n",
        "1. D_c = 'event'\n",
        "2. Yes (because the ordered pair (D_c, D_i) = ('event', 'event') is valid)\n",
        "3. Yes\n",
        "4. True. Because 1) \"PersonX learns to play the trumpet\" means PersonX are talent and effortful in learning musical instrument, and 2) people tend to applaud other people's effort and talent.\n",
        "\n",
        "* Example 2\n",
        "Inference: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "D_i = 'mental state', H = 'PersonX do not feel well', T = 'PersonX only make it through half'\n",
        "1. D_c = 'event'\n",
        "2. No (because the ordered pair (D_c, D_i) = ('event', 'mental state') is not valid)\n",
        "3. Yes\n",
        "4. False. Because, result of 2 is No.\n",
        "\n",
        "Now, please do your task for (each of) the following statement(s):\n",
        "Inference: {}\n",
        "D_i = '{}', H = '{}', T = '{}'\"\"\"\n",
        "\n",
        "conceptualization_prompt_batch_input = \"\"\"Please read the convention, task description, examples, then do the task:\n",
        "Convention: There are three dimensions ['persona', 'event', 'mental state']. All ordered dimension pairs are valid, except ('event', 'persona') and ('event', 'mental state') are invalid.\n",
        "\n",
        "Task description: Given an inference and its corresponding dimension D_i and its two main clauses H and T, you are required to do the following step by step:\n",
        "1. Focus on the last clause T, determine which dimension best describe what the clause expresses. Denote the dimension as D_c.\n",
        "2. Judge if the ordered pair (D_c, D_i) is valid. Answer 'Yes' or 'No'.\n",
        "3. Judge if the main clauses' meaning is all clear/non-ambiguous. Answer 'Yes' or 'No'.\n",
        "4. Based on result in 2, 3, and the semantics of the inference, judge if the inference is likely 'True' or 'False'. Note that if result of either 2 or 3 is 'No', then result of 4 should be 'False'.\n",
        "\n",
        "Examples:\n",
        "* Example 1\n",
        "Inference: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent,\n",
        "D_i = 'event', H = 'PersonX learns to play the trumpet', T = 'PersonY wants to applaud PersonX's talent'\n",
        "1. D_c = 'event'\n",
        "2. Yes (because the ordered pair (D_c, D_i) = ('event', 'event') is valid)\n",
        "3. Yes\n",
        "4. True. Because 1) \"PersonX learns to play the trumpet\" means PersonX are talent and effortful in learning musical instrument, and 2) people tend to applaud other people's effort and talent.\n",
        "\n",
        "* Example 2\n",
        "Inference: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.,\n",
        "D_i = 'mental state', H = 'PersonX do not feel well', T = 'PersonX only make it through half'\n",
        "1. D_c = 'event'\n",
        "2. No (because the ordered pair (D_c, D_i) = ('event', 'mental state') is not valid)\n",
        "3. Yes\n",
        "4. False. Because, result of 2 is No.\n",
        "\n",
        "Now, please do your task for (each of) the following statement(s):\n",
        "\"\"\"\n",
        "conceptualization_prompt_batch_input_tail = \"Inference: {}, D_i = '{}', H = '{}', T = '{}'\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kosRxmUAPI6n"
      },
      "source": [
        "gpt-3.5-turbo is so stupid in following rule! Need to take the rule explicit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgDUb0Upc2c1"
      },
      "outputs": [],
      "source": [
        "conceptualization_prompt_chatgpt = \"\"\"Please read the convention, task description, examples, then do the task:\n",
        "Convention: There are three dimensions ['persona', 'event', 'mental state'].\n",
        "\n",
        "Task description: Given an inference and its corresponding dimension D_i and its two main clauses H and T, you are required to do the following step by step:\n",
        "1. Focus on the last clause T, determine which dimension best describe what the clause expresses. Denote the dimension as D_c.\n",
        "2. If the ordered pair (D_c, D_i) is ('event', 'persona') or ('event', 'mental state') then write 'No', otherwise write 'Yes'.\n",
        "3. Judge if the main clauses' meaning is all clear/non-ambiguous. Answer 'Yes' or 'No'.\n",
        "4. Based on result in 2, 3, and the semantics of the inference, judge if the inference is likely 'True' or 'False'. Note that if result of either 2 or 3 is 'No', then result of 4 should be 'False'.\n",
        "\n",
        "Examples:\n",
        "* Example 1\n",
        "Inference: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent\n",
        "D_i = 'event', H = 'PersonX learns to play the trumpet', T = 'PersonY wants to applaud PersonX's talent'\n",
        "1. D_c = 'event'\n",
        "2. Yes\n",
        "3. Yes\n",
        "4. True. Because 1) \"PersonX learns to play the trumpet\" means PersonX are talent and effortful in learning musical instrument, and 2) people tend to applaud other people's effort and talent.\n",
        "\n",
        "* Example 2\n",
        "Inference: PersonX eat most of those, thus it can be seen about PersonX's intention that, PersonX love curly fries\n",
        "D_i = 'event', H = 'PersonX eat most of those', T = 'PersonX love curly fries'\n",
        "1. D_c = 'mental state'\n",
        "2. Yes\n",
        "3. Yes\n",
        "4. False. Because 'love curly fries' expresses PersonX's preference rather than intention.\n",
        "\n",
        "* Example 3\n",
        "Inference: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "D_i = 'mental state', H = 'PersonX do not feel well', T = 'PersonX only make it through half'\n",
        "1. D_c = 'event'\n",
        "2. No\n",
        "3. Yes\n",
        "4. False. Because, result of 2 is No.\n",
        "\n",
        "* Example 4\n",
        "Inference: Before PersonX really empathize with PersonY, PersonX feel.\n",
        "D_i = 'event', H = 'PersonX really empathize with PersonY', T = 'PersonX feel'\n",
        "1. D_c = 'mental state'\n",
        "2. Yes\n",
        "3. No (T is unclear about what 'PersonX feel')\n",
        "4. False. Because result of 3 is No.\n",
        "\n",
        "* Example 5\n",
        "Inference: PersonX have a traumatic incident, thus it can be seen about PersonX's attribute that PersonX be introvert\n",
        "D_i = 'persona', H = 'PersonX have a traumatic incident', T = 'PersonX be introvert'\n",
        "1. D_c = 'persona'\n",
        "2. Yes\n",
        "3. Yes\n",
        "4. False. Because having a traumatic incident likely result from unfortune or carelessness, while being introvert is not relevant.\n",
        "\n",
        "Now, please do your task for (each of) the following statement(s):\n",
        "Inference: {}\n",
        "D_i = '{}', H = '{}', T = '{}'\"\"\"\n",
        "\n",
        "conceptualization_prompt_chatgpt_2 = \"\"\"Please read the task description and examples, then do the task:\n",
        "Task description: Given an inference, its aspect D, and its two main clauses H and T, you are required to do the following step by step:\n",
        "1. Determine if clause T expresses about a/an D of the subject. Answer 'Yes' or 'No'.\n",
        "2. Judge if the meanings of H and T are all clear/non-ambiguous. Answer 'Yes' or 'No'.\n",
        "3. If result of either 1 or 2 is 'No', please answer 'False'. Otherwise, judge if the inference is likely 'True' or 'False'.\n",
        "\n",
        "Examples:\n",
        "* Example 1\n",
        "Inference: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent\n",
        "D = 'event', H = 'PersonX learns to play the trumpet', T = 'PersonY wants to applaud PersonX's talent'\n",
        "1. Yes. Because 'PersonY wants to applaud PersonX's talent' expresses an 'event' of PersonY\n",
        "2. Yes.\n",
        "3. True. Because 1) \"PersonX learns to play the trumpet\" means PersonX are talent and effortful in learning musical instrument, and 2) people tend to applaud other people's effort and talent.\n",
        "\n",
        "* Example 2\n",
        "Inference: PersonX eat most of those, thus as a result on PersonX's emotion, PersonX feel hungry\n",
        "D = 'mental state', H = 'PersonX eat most of those', T = 'PersonX feel hungry'\n",
        "1. Yes. Because 'PersonX feel hungry' expresses a 'mental state' of PersonX\n",
        "2. Yes.\n",
        "3. False. Because it's unlikely that PersonX still feel hungry after eat a lot.\n",
        "\n",
        "* Example 3\n",
        "Inference: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "D = 'mental state', H = 'PersonX do not feel well', T = 'PersonX only make it through half'\n",
        "1. No. Because 'PersonX only make it through half' doesn't expresses an 'mental state' of PersonX\n",
        "2. Yes.\n",
        "3. False. Because, result of 1 is No.\n",
        "\n",
        "* Example 4\n",
        "Inference: Before PersonX really empathize with PersonY, PersonX feel.\n",
        "D = 'event', H = 'PersonX really empathize with PersonY', T = 'PersonX feel'\n",
        "1. Yes. Because 'PersonX feel' expresses an 'event' of PersonX\n",
        "2. No. Because the meaning of 'PersonX feel' is unclear/ambiguous\n",
        "3. False. Because result of 2 is No.\n",
        "\n",
        "* Example 5\n",
        "Inference: PersonX have a traumatic incident, thus it can be seen about PersonX's attribute that PersonX be introvert\n",
        "D = 'persona', H = 'PersonX have a traumatic incident', T = 'PersonX be introvert'\n",
        "1. Yes. Because 'PersonX be introvert' expresses a persona of PersonX\n",
        "2. Yes.\n",
        "3. False. Because having a traumatic incident likely result from unfortune or carelessness, while being introvert is not relevant.\n",
        "\n",
        "Now, please do your task for (each of) the following statement(s):\n",
        "Inference: {}\n",
        "D = '{}', H = '{}', T = '{}'\"\"\"\n",
        "\n",
        "conceptualization_prompt_chatgpt_symbolic = \"\"\"Please read the convention, task description, examples, then do the task:\n",
        "Convention: There are three dimensions S = ['persona', 'event', 'mental state'] and a binary operator R: S x S as follow\n",
        "R('persona', 'persona') = R('persona', 'event') = R('persona', 'mental state') = R('event', 'event') = R('mental state', 'persona') = R('mental state', 'event') = R('mental state', 'mental state') = 'Yes'\n",
        "R('event', 'persona') = R('event', 'mental state') = 'No'\n",
        "\n",
        "Task description: Given an inference and its corresponding dimension D_i and its two main clauses H and T, you are required to do the following step by step:\n",
        "1. Focus on the last clause T, determine which dimension best describe what the clause expresses. Denote the dimension as D_c.\n",
        "2. Compute R(D_c, D_i)\n",
        "3. Judge if the main clauses' meaning is all clear/non-ambiguous. Answer 'Yes' or 'No'.\n",
        "4. Based on result in 2, 3, and the semantics of the inference, judge if the inference is likely 'True' or 'False'. Note that if result of either 2 or 3 is 'No', then result of 4 should be 'False'.\n",
        "\n",
        "Examples:\n",
        "* Example 1\n",
        "Inference: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent\n",
        "D_i = 'event', H = 'PersonX learns to play the trumpet', T = 'PersonY wants to applaud PersonX's talent'\n",
        "1. D_c = 'event'\n",
        "2. R(D_c, D_i) = R('event', 'event') = Yes\n",
        "3. Yes\n",
        "4. True. Because 1) \"PersonX learns to play the trumpet\" means PersonX are talent and effortful in learning musical instrument, and 2) people tend to applaud other people's effort and talent.\n",
        "\n",
        "* Example 2\n",
        "Inference: PersonX eat most of those, thus it can be seen about PersonX's intention that, PersonX love curly fries\n",
        "D_i = 'event', H = 'PersonX eat most of those', T = 'PersonX love curly fries'\n",
        "1. D_c = 'mental state'\n",
        "2. R(D_c, D_i) = R('mental state', 'event') = Yes\n",
        "3. Yes\n",
        "4. False. Because 'love curly fries' expresses PersonX's preference rather than intention.\n",
        "\n",
        "* Example 3\n",
        "Inference: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "D_i = 'mental state', H = 'PersonX do not feel well', T = 'PersonX only make it through half'\n",
        "1. D_c = 'event'\n",
        "2. R(D_c, D_i) = R('event', 'mental state') = No\n",
        "3. Yes\n",
        "4. False. Because, result of 2 is No.\n",
        "\n",
        "* Example 4\n",
        "Inference: Before PersonX really empathize with PersonY, PersonX feel.\n",
        "D_i = 'event', H = 'PersonX really empathize with PersonY', T = 'PersonX feel'\n",
        "1. D_c = 'mental state'\n",
        "2. R(D_c, D_i) = R('mental state', 'event') = Yes\n",
        "3. No (T is unclear about what 'PersonX feel')\n",
        "4. False. Because result of 3 is No.\n",
        "\n",
        "* Example 5\n",
        "Inference: PersonX have a traumatic incident, thus it can be seen about PersonX's attribute that PersonX be introvert\n",
        "D_i = 'persona', H = 'PersonX have a traumatic incident', T = 'PersonX be introvert'\n",
        "1. D_c = 'persona'\n",
        "2. R(D_c, D_i) = R('persona', 'persona') = Yes\n",
        "3. Yes\n",
        "4. False. Because having a traumatic incident likely result from unfortune or carelessness, while being introvert is not relevant.\n",
        "\n",
        "\n",
        "Now, please do your task for (each of) the following statement(s):\n",
        "Inference: {}\n",
        "D_i = '{}', H = '{}', T = '{}'\"\"\"\n",
        "\n",
        "\n",
        "conceptualization_prompt_chatgpt_2_different_question = \"\"\"Please read the task description and examples, then do the task:\n",
        "Task description: Given an inference, its aspect D, and its two main clauses H and T, you are required to do the following step by step:\"\"\"\n",
        "1. Determine if clause T expresses about a/an D of the subject. Answer 'Yes' or 'No'. # unclear question! Need to do analysis\n",
        "\"\"\"\n",
        "2. Judge if the meanings of H and T are all clear/non-ambiguous. Answer 'Yes' or 'No'.\n",
        "3. If result of either 1 or 2 is 'No', please answer 'False'. Otherwise, judge if the inference is likely 'True' or 'False'.\n",
        "\n",
        "Examples:\n",
        "* Example 1\n",
        "Inference: PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent\n",
        "D = 'event', H = 'PersonX learns to play the trumpet', T = 'PersonY wants to applaud PersonX's talent'\n",
        "1. Yes. Because 'PersonY wants to applaud PersonX's talent' expresses an 'event' of PersonY\n",
        "2. Yes.\n",
        "3. True. Because 1) \"PersonX learns to play the trumpet\" means PersonX are talent and effortful in learning musical instrument, and 2) people tend to applaud other people's effort and talent.\n",
        "\n",
        "* Example 2\n",
        "Inference: PersonX eat most of those, thus as a result on PersonX's emotion, PersonX feel hungry\n",
        "D = 'mental state', H = 'PersonX eat most of those', T = 'PersonX feel hungry'\n",
        "1. Yes. Because 'PersonX feel hungry' expresses a 'mental state' of PersonX\n",
        "2. Yes.\n",
        "3. False. Because it's unlikely that PersonX still feel hungry after eat a lot.\n",
        "\n",
        "* Example 3\n",
        "Inference: PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.\n",
        "D = 'mental state', H = 'PersonX do not feel well', T = 'PersonX only make it through half'\n",
        "1. No. Because 'PersonX only make it through half' doesn't expresses an 'mental state' of PersonX\n",
        "2. Yes.\n",
        "3. False. Because, result of 1 is No.\n",
        "\n",
        "* Example 4\n",
        "Inference: Before PersonX really empathize with PersonY, PersonX feel.\n",
        "D = 'event', H = 'PersonX really empathize with PersonY', T = 'PersonX feel'\n",
        "1. Yes. Because 'PersonX feel' expresses an 'event' of PersonX\n",
        "2. No. Because the meaning of 'PersonX feel' is unclear/ambiguous\n",
        "3. False. Because result of 2 is No.\n",
        "\n",
        "* Example 5\n",
        "Inference: PersonX have a traumatic incident, thus it can be seen about PersonX's attribute that PersonX be introvert\n",
        "D = 'persona', H = 'PersonX have a traumatic incident', T = 'PersonX be introvert'\n",
        "1. Yes. Because 'PersonX be introvert' expresses a persona of PersonX\n",
        "2. Yes.\n",
        "3. False. Because having a traumatic incident likely result from unfortune or carelessness, while being introvert is not relevant.\n",
        "\n",
        "Now, please do your task for (each of) the following statement(s):\n",
        "Inference: {}\n",
        "D = '{}', H = '{}', T = '{}'\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URi8cSFuEcjI"
      },
      "source": [
        "concept template 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZMY83RJEeWx"
      },
      "outputs": [],
      "source": [
        "bs = 102 # len(data_df)\n",
        "predictions = []\n",
        "\n",
        "# bs = 1; for b in [38]:\n",
        "for b in range(len(data_df)//bs):\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [\n",
        "                [{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": conceptualization_prompt_chatgpt_2.format(\n",
        "                    data_df['assertion'][i],\n",
        "                    data_df['dim'][i],\n",
        "                    data_df['head'][i],\n",
        "                    data_df['tail'][i])\n",
        "                }]\n",
        "                for i in range(b*bs, min((b+1)*bs, len(data_df)))\n",
        "            ],\n",
        "            max_tokens=100,\n",
        "        )\n",
        "    print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    predictions.extend([x['choices'] for x in pred])\n",
        "    time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ntfpc5JF9sA"
      },
      "outputs": [],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA3Dq0YLEYcT"
      },
      "source": [
        "concept template 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "burAUF171p89"
      },
      "outputs": [],
      "source": [
        "bs = 102 # len(data_df)\n",
        "predictions = []\n",
        "\n",
        "# bs = 1; for b in [38]:\n",
        "for b in range(len(data_df)//bs):\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [\n",
        "                [{\n",
        "                \"role\": \"user\",\n",
        "                \"content\": conceptualization_prompt_chatgpt.format(\n",
        "                    data_df['assertion'][i],\n",
        "                    data_df['dim'][i],\n",
        "                    data_df['head'][i],\n",
        "                    data_df['tail'][i])\n",
        "                }]\n",
        "                for i in range(b*bs, min((b+1)*bs, len(data_df)))\n",
        "            ],\n",
        "            max_tokens=100,\n",
        "        )\n",
        "    print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    predictions.extend([x['choices'] for x in pred])\n",
        "    time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPegTZF5Ptsj"
      },
      "outputs": [],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2SJcw51PVv4"
      },
      "outputs": [],
      "source": [
        "for i, s in enumerate(predictions):\n",
        "    answer = ''\n",
        "    s = s[0]['message']['content']\n",
        "    for l in s.split('\\n'):\n",
        "        if l.startswith('4.'):\n",
        "            answer = l[3:]\n",
        "            break\n",
        "    print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcXpLxXJPVhw"
      },
      "source": [
        "symbolic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lu60ypL0B-Pw"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "bs = 1\n",
        "\n",
        "for i in range((len(data)-1)//bs + 1):\n",
        "    p = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": conceptualization_prompt_chatgpt_symbolic.format(\n",
        "            data_df['assertion'][i],\n",
        "            data_df['dim'][i],\n",
        "            data_df['head'][i],\n",
        "            data_df['tail'][i])\n",
        "        }],\n",
        "        max_tokens=100,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(data_df['assertion'][i])\n",
        "    print(p['choices'][0]['message']['content'])\n",
        "    predictions.extend(p['choices'])\n",
        "    time.sleep(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZeeWDMkMA1q"
      },
      "outputs": [],
      "source": [
        "for i, s in enumerate(predictions):\n",
        "    answer = ''\n",
        "    s = s['message']['content']\n",
        "    for l in s.split('\\n'):\n",
        "        if l.startswith('4.'):\n",
        "            answer = l[3:]\n",
        "            break\n",
        "    print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4H77GHRB9Ap"
      },
      "source": [
        "testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzyaYr-LNw7E"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "bs = 1\n",
        "\n",
        "for i in [38]:\n",
        "# for i in range((len(data)-1)//bs + 1):\n",
        "    p = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0301\",\n",
        "        messages=[{\"role\": \"user\", \"content\": conceptualization_prompt_chatgpt.format(\n",
        "            data_df['assertion'][i],\n",
        "            data_df['dim'][i],\n",
        "            data_df['head'][i],\n",
        "            data_df['tail'][i])\n",
        "        }],\n",
        "        max_tokens=100,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(p['choices'])\n",
        "    predictions.extend(p['choices'])\n",
        "    break\n",
        "    time.sleep(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMQFg2Ngs80W"
      },
      "outputs": [],
      "source": [
        "pred = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    messages=[\n",
        "        # {\"role\": \"system\", \"content\": role},\n",
        "        {\"role\": \"user\", \"content\": conceptualization_prompt.format(data[34], 'mental state', 'PersonX prevent PersonY', 'PersonX never reach out to anyone')} # + \"\\n\\nLet's think step by step\"},\n",
        "    ],\n",
        "    # max_tokens=4,\n",
        "    temperature=0\n",
        ")\n",
        "print(pred['choices'][0]['message']['content'])\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgcYS-hRMhEt"
      },
      "outputs": [],
      "source": [
        "{\"role\": \"user\", \"content\": conceptualization_prompt.format(data[34], 'emotion')} # + \"\\n\\nLet's think step by step\"},\n",
        "\n",
        "# 1. action\n",
        "# 2. Yes (because the aspect-dimension pair ('action', 'emotion') is valid)\n",
        "# 3. No\n",
        "# 4. Likely True. Because the inference suggests that PersonX's emotion is negatively affected by preventing PersonY, which may lead to a lack of desire to reach out to anyone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9m2PxFoxiBA"
      },
      "outputs": [],
      "source": [
        "bs = 102\n",
        "predictions = []\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(data)//bs):\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": conceptualization_prompt.format(s)}] for s in data[i*bs:(i+1)*bs]],\n",
        "            max_tokens=100,\n",
        "        )\n",
        "    print('\\n\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "    time.sleep(10)\n",
        "\n",
        "end = time.time()\n",
        "print('Running time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb3glfMNn2HC"
      },
      "outputs": [],
      "source": [
        "for i, s in enumerate(predictions):\n",
        "    answer = ''\n",
        "    for l in s.split('\\n'):\n",
        "        if l.startswith('4.'):\n",
        "            answer = l\n",
        "            break\n",
        "    print(i, answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqtwhKHK8rcb"
      },
      "outputs": [],
      "source": [
        "print(gen_chatgpt['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjPL4aZwIslI"
      },
      "source": [
        "### MoE\n",
        "\n",
        "Mixture of Expert or Mixture of Constraint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coWI_dcPIr9s"
      },
      "outputs": [],
      "source": [
        "moe_q1_prompt1 = \"Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause '{}' express. Answer the choice only.\"\n",
        "moe_q1_prompt2 = \"Determine if clause '{}' expresses an event or activity of the subject. Answer 'Yes' or 'No' only.\"\n",
        "\n",
        "moe_q2_prompt1 = \"Judge if the meaning of the clauses '{}' and '{}' are all clear. Answer 'Yes' or 'No' only.\"\n",
        "moe_q2_prompt2 = \"\"\"Which one of the following two statements make more sense:\n",
        "0. Two clauses '{0}' and '{1}' all have clear meaning.\n",
        "1. One of two following clauses '{0}' and '{1}' has ambiguous meaning.\n",
        "Answer 0 or 1 only.\"\"\"\n",
        "\n",
        "moe_q3_prompt1 = \"Judge if the event '{}' likely occurs after the event '{}'. Answer 'Yes' or 'No' only.\"    # pretty unstable\n",
        "moe_q3_prompt1_reverse = \"Judge if the event '{}' likely occurs before the event '{}'. Answer 'Yes' or 'No' only.\"   # pretty unstable\n",
        "moe_q3_prompt2 = \"Given 2 events in a story: A. '{}', B. '{}'. Is A likely to occur after B? Answer 'Yes' or 'No' only.\"\n",
        "moe_q3_prompt3 = \"\"\"Which one of the following two statements is more plausible:\n",
        "0. {0} before {1}\n",
        "1. {0} after {1}\n",
        "Answer 0 or 1 only.\"\"\" # tail [] head, 1 -> after, 0 -> before\n",
        "\n",
        "moe_q4_prompt1 = \"Judge if the event '{}' causes the event '{}'. Answer 'Yes' or 'No' only.\" # need to observe on dev set. Not easy to judge, thus not System 1 thinking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXzDCBz6PxmD"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "s = data_df['tail'][i]\n",
        "pred = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    messages=[{\"role\": \"user\", \"content\": moe_q1_prompt1.format(s)}],\n",
        "    max_tokens=4, temperature=0\n",
        ")\n",
        "\n",
        "print(s)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2BoZR2bOu1T"
      },
      "outputs": [],
      "source": [
        "bs = 10\n",
        "for i in range(0, len(data)//bs+1):\n",
        "    print(i)\n",
        "    pred = await dispatch_openai_requests(\n",
        "            messages_list = [[{\"role\": \"user\", \"content\": moe_q1_prompt1.format(s)}] for s in data_df['tail'][i*bs:(i+1)*bs]],\n",
        "            max_tokens=4,\n",
        "        )\n",
        "    print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "    predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "    time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI5G0QVGVII2"
      },
      "outputs": [],
      "source": [
        "bs = 5\n",
        "for i in range(0, len(data)//bs+1):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q2_prompt1.format(s)}] for s in data_df['tail'][i*bs:(i+1)*bs]],\n",
        "                    max_tokens=1,\n",
        "                )\n",
        "            print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(1)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD2-fVIZyLYZ"
      },
      "outputs": [],
      "source": [
        "bs = 5\n",
        "for i in range(0, len(data)//bs+1):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q2_prompt1.format(s)}] for s in data_df['head'][i*bs:(i+1)*bs]],\n",
        "                    max_tokens=1,\n",
        "                )\n",
        "            print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(1)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-E0HBaWiAov",
        "outputId": "4d632bdb-4820-403d-b39a-b91879245c2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Which one of the following two statements is more plausible:\n",
            "0. PersonX belief be true before PersonX believe thing\n",
            "1. PersonX belief be true after PersonX believe thing\n",
            "Answer 0 or 1 only.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-7JP1BtxcUdJLHqVRnp8zrmYaqag4X at 0x7fbd28254040> JSON: {\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"finish_reason\": \"stop\",\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"content\": \"As an AI language model, I cannot determine the likelihood of an event occurring. However, if PersonX believes something, it is possible that their belief is true or false. Therefore, the answer is 'uncertain'.\",\n",
              "        \"role\": \"assistant\"\n",
              "      }\n",
              "    }\n",
              "  ],\n",
              "  \"created\": 1684858757,\n",
              "  \"id\": \"chatcmpl-7JP1BtxcUdJLHqVRnp8zrmYaqag4X\",\n",
              "  \"model\": \"gpt-3.5-turbo-0301\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"usage\": {\n",
              "    \"completion_tokens\": 44,\n",
              "    \"prompt_tokens\": 38,\n",
              "    \"total_tokens\": 82\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "i = 60 # 61 or 68\n",
        "pred = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    messages=[{\"role\": \"user\", \"content\": moe_q3_prompt1.format(data_df['tail'][i], data_df['head'][i])}],\n",
        "    max_tokens=50, temperature=0\n",
        ")\n",
        "\n",
        "print(moe_q3_prompt3.format(data_df['tail'][i], data_df['head'][i]))\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVwWV9_79Z9L"
      },
      "outputs": [],
      "source": [
        "pred = await dispatch_openai_requests(\n",
        "        messages_list = [[{\"role\": \"user\", \"content\": moe_q3_prompt3.format(data_df['tail'][i], data_df['head'][i])}] \\\n",
        "                         for i in range(0, 102)\n",
        "                         ],\n",
        "        max_tokens=4,\n",
        "    )\n",
        "print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZioCcbpC852M"
      },
      "outputs": [],
      "source": [
        "bs = 5\n",
        "for i in range(0, len(data)//bs+1):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/')\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q3_prompt3.format(data_df['tail'][b], data_df['head'][b])}] \\\n",
        "                                     for b in range(i*bs, min((i+1)*bs, len(data_df)))\n",
        "                                    ],\n",
        "                    max_tokens=4,\n",
        "                )\n",
        "            print('\\n'.join([x['choices'][0]['message']['content'] for x in pred]))\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(1)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KATE"
      ],
      "metadata": {
        "id": "y8OfdFRnHSlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kate_prompt_0 = \"Answer whether the following statements plausible. Answer with only Yes or No:\\n{}\"\n",
        "kate_prompt_1 = \"Judge the following statements if they are likely to occur, only answer 'True' or 'False':\\n{}\""
      ],
      "metadata": {
        "id": "EM1KPD-rODDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = 22\n",
        "exemplars = '\\n\\n'.join([\n",
        "    'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0_tq'][s], 'Yes'),\n",
        "    'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0_tq'][s], 'No'),\n",
        "    'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion_tq'][s], ''),\n",
        "])\n",
        "print(kate_prompt_0.format(exemplars))\n",
        "pred = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    messages=[{\"role\": \"user\", \"content\": kate_prompt_0.format(exemplars)}],\n",
        "    max_tokens=2, temperature=0\n",
        ")\n",
        "pred['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "qGDOf4BeV4g_",
        "outputId": "c29c5ad7-1b23-4d0b-f1bf-c92b1a1e19da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer whether the following statements plausible. Answer with only Yes or No:\n",
            "Statement: If PersonX come early, as a result, PersonX will, PersonX go home\n",
            "Answer: Yes\n",
            "\n",
            "Statement: If PersonX go home early, as a result, PersonX will, PersonX come day\n",
            "Answer: No\n",
            "\n",
            "Statement: If PersonX start early, as a result, PersonX will, PersonX go home early\n",
            "Answer: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 22\n",
        "exemplars = '\\n\\n'.join([\n",
        "    'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0'][s], 'Yes'),\n",
        "    'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0'][s], 'No'),\n",
        "    'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], ''),\n",
        "])\n",
        "print(kate_prompt_0.format(exemplars))\n",
        "pred = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    messages=[{\"role\": \"user\", \"content\": kate_prompt_0.format(exemplars)}],\n",
        "    max_tokens=2, temperature=0\n",
        ")\n",
        "pred['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "HQU9iK_bOr_y",
        "outputId": "e83fc45f-7228-4ee7-e14b-6f6b8f2ec7e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer whether the following statements plausible. Answer with only Yes or No:\n",
            "Statement: PersonX come early, thus as an result, PersonX go home\n",
            "Answer: Yes\n",
            "\n",
            "Statement: PersonX go home early, thus as an result, PersonX come day\n",
            "Answer: No\n",
            "\n",
            "Statement: PersonX start early, thus as an result, PersonX go home early\n",
            "Answer: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 22\n",
        "exemplars = '\\n\\n'.join([\n",
        "    'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0'][s], 'Yes'),\n",
        "    'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0'][s], 'No'),\n",
        "    'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], ''),\n",
        "])\n",
        "print(kate_prompt_1.format(exemplars))\n",
        "pred = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    messages=[{\"role\": \"user\", \"content\": kate_prompt_0.format(exemplars)}],\n",
        "    max_tokens=2, temperature=0\n",
        ")\n",
        "pred['choices'][0]['message']['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "t6NV_kAaVzeX",
        "outputId": "51c64739-12e8-4362-8042-045862bb9f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Judge the following statements if they are likely to occur, only answer 'True' or 'False':\n",
            "Statement: PersonX come early, thus as an result, PersonX go home\n",
            "Answer: Yes\n",
            "\n",
            "Statement: PersonX go home early, thus as an result, PersonX come day\n",
            "Answer: No\n",
            "\n",
            "Statement: PersonX start early, thus as an result, PersonX go home early\n",
            "Answer: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in [0, 22]:\n",
        "    exemplars = '\\n\\n'.join([\n",
        "        'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0_tq'][s], 'Yes'),\n",
        "        'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0_tq'][s], 'No'),\n",
        "        'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion_tq'][s], ''),\n",
        "    ])\n",
        "    print(kate_prompt_0.format(exemplars))\n",
        "\n",
        "    pred = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=[kate_prompt_0.format(exemplars)],\n",
        "        max_tokens=4, temperature=0\n",
        "    )\n",
        "    print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_fUmzqtcDic",
        "outputId": "6c3e57fd-d7c7-49d8-c568-955bcf6912b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer whether the following statements plausible. Answer with only Yes or No:\n",
            "Statement: PersonX be real, thus as an result, PersonY argue PersonZ about it\n",
            "Answer: Yes\n",
            "\n",
            "Statement: PersonX come to talk to PersonZ, thus as an result, PersonY become enraged\n",
            "Answer: No\n",
            "\n",
            "Statement: If PersonX argue with PersonY, as a result, PersonY or others will, PersonY be not a PersonZ\n",
            "Answer: \n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" No\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1685531442,\n",
            "  \"id\": \"cmpl-7ME0wp301i7vJFzKrirJKeTere0jl\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 1,\n",
            "    \"prompt_tokens\": 97,\n",
            "    \"total_tokens\": 98\n",
            "  }\n",
            "}\n",
            "Answer whether the following statements plausible. Answer with only Yes or No:\n",
            "Statement: PersonX come early, thus as an result, PersonX go home\n",
            "Answer: Yes\n",
            "\n",
            "Statement: PersonX go home early, thus as an result, PersonX come day\n",
            "Answer: No\n",
            "\n",
            "Statement: If PersonX start early, as a result, PersonX will, PersonX go home early\n",
            "Answer: \n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" Yes\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1685531443,\n",
            "  \"id\": \"cmpl-7ME0x3SpjZ1Zfpuxd5MCGuYtjQADp\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 1,\n",
            "    \"prompt_tokens\": 85,\n",
            "    \"total_tokens\": 86\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in [0, 22]:\n",
        "    exemplars = '\\n\\n'.join([\n",
        "        'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0'][s], 'Yes'),\n",
        "        'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0'][s], 'No'),\n",
        "        'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], ''),\n",
        "    ])\n",
        "    print(kate_prompt_0.format(exemplars))\n",
        "\n",
        "    pred = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=[kate_prompt_0.format(exemplars)],\n",
        "        max_tokens=4, temperature=0\n",
        "    )\n",
        "    print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esraWHKvczE0",
        "outputId": "ab9c4e17-f537-4e6d-e2fc-3b0471541638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer whether the following statements plausible. Answer with only Yes or No:\n",
            "Statement: PersonX be real, thus as an result, PersonY argue PersonZ about it\n",
            "Answer: Yes\n",
            "\n",
            "Statement: PersonX come to talk to PersonZ, thus as an result, PersonY become enraged\n",
            "Answer: No\n",
            "\n",
            "Statement: PersonX argue with PersonY, thus as an result, PersonY be not a PersonZ\n",
            "Answer: \n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" No\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1685531441,\n",
            "  \"id\": \"cmpl-7ME0v6gOYsmVodzMVPUmt2PjBrtVN\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 1,\n",
            "    \"prompt_tokens\": 91,\n",
            "    \"total_tokens\": 92\n",
            "  }\n",
            "}\n",
            "Answer whether the following statements plausible. Answer with only Yes or No:\n",
            "Statement: PersonX come early, thus as an result, PersonX go home\n",
            "Answer: Yes\n",
            "\n",
            "Statement: PersonX go home early, thus as an result, PersonX come day\n",
            "Answer: No\n",
            "\n",
            "Statement: PersonX start early, thus as an result, PersonX go home early\n",
            "Answer: \n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" Yes\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1685531442,\n",
            "  \"id\": \"cmpl-7ME0wLdITcMSa92JTwzcvWpkiMJBJ\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 1,\n",
            "    \"prompt_tokens\": 81,\n",
            "    \"total_tokens\": 82\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in [0, 22]:\n",
        "    exemplars = '\\n\\n'.join([\n",
        "        'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0'][s], 'True'),\n",
        "        'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0'][s], 'False'),\n",
        "        'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], ''),\n",
        "    ])\n",
        "    print(kate_prompt_1.format(exemplars))\n",
        "\n",
        "    pred = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=[kate_prompt_0.format(exemplars)],\n",
        "        max_tokens=4, temperature=0\n",
        "    )\n",
        "    print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6LAWBFlczu0",
        "outputId": "f543b990-e523-440a-9dea-6986f3a33eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Judge the following statements if they are likely to occur, only answer 'True' or 'False':\n",
            "Statement: PersonX be real, thus as an result, PersonY argue PersonZ about it\n",
            "Answer: True\n",
            "\n",
            "Statement: PersonX come to talk to PersonZ, thus as an result, PersonY become enraged\n",
            "Answer: False\n",
            "\n",
            "Statement: PersonX argue with PersonY, thus as an result, PersonY be not a PersonZ\n",
            "Answer: \n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" False\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1685531444,\n",
            "  \"id\": \"cmpl-7ME0yPVAUcRvwZJHqGbeEJjxGIwuY\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 1,\n",
            "    \"prompt_tokens\": 91,\n",
            "    \"total_tokens\": 92\n",
            "  }\n",
            "}\n",
            "Judge the following statements if they are likely to occur, only answer 'True' or 'False':\n",
            "Statement: PersonX come early, thus as an result, PersonX go home\n",
            "Answer: True\n",
            "\n",
            "Statement: PersonX go home early, thus as an result, PersonX come day\n",
            "Answer: False\n",
            "\n",
            "Statement: PersonX start early, thus as an result, PersonX go home early\n",
            "Answer: \n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"text\": \" Yes\"\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1685531445,\n",
            "  \"id\": \"cmpl-7ME0z0OsC63wv3B8yxD4hkjh8mdE2\",\n",
            "  \"model\": \"text-davinci-003\",\n",
            "  \"object\": \"text_completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 1,\n",
            "    \"prompt_tokens\": 81,\n",
            "    \"total_tokens\": 82\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Least to Most"
      ],
      "metadata": {
        "id": "sNvOVwE0hqV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# where to get these examples -> from dev set please!\n",
        "exemplars = {\n",
        "    \"xReact\": [\n",
        "        [\n",
        "            \"PersonX prevent PersonY, thus as a result on PersonX's emotion, PersonX never reach out to anyone\",\n",
        "            \"PersonX never reach out to anyone\"\n",
        "        ],\n",
        "        [\n",
        "            \"PersonX go to sleep on hollow, thus as a result on PersonX's emotion, PersonX feel PersonX be tired\",\n",
        "            \"PersonX feel PersonX be tired\",\n",
        "            \"3. mental state\",\n",
        "            \"PersonX go to sleep on hollow, thus as a result on PersonX's emotion, PersonX feel PersonX be tired\"\n",
        "        ],\n",
        "        [\n",
        "            \"PersonX eat the sub, thus as a result on PersonX's emotion, PersonX feel PersonX be full\",\n",
        "            \"PersonX feel PersonX be full\",\n",
        "            \"3. mental state\",\n",
        "            \"PersonX eat the sub, thus as a result on PersonX's emotion, PersonX feel PersonX be full\"\n",
        "        ]\n",
        "    ],\n",
        "    \"oReact\": [\n",
        "        [\n",
        "            \"PersonX bear PersonY, thus as a result on PersonY's emotion, PersonY feel PersonY be here\",\n",
        "            \"PersonY be here\"\n",
        "        ],\n",
        "        [\n",
        "            \"PersonX play ana, thus as a result on PersonY's emotion, PersonY feel PersonY be fun\",\n",
        "            \"PersonY be fun\",\n",
        "            \"3. mental state\",\n",
        "            \"PersonX play ana, thus as a result on PersonY's emotion, PersonY feel PersonY be fun\"\n",
        "        ],\n",
        "        [\n",
        "            \"PersonX gaze at PersonY, thus as a result on PersonY's emotion, PersonY blush\",\n",
        "            \"PersonY blush\",\n",
        "            \"3. mental state\",\n",
        "            \"PersonX gaze at PersonY, thus as a result on PersonY's emotion, PersonY blush\"\n",
        "        ]\n",
        "    ],\n",
        "    \"xAttr\": [\n",
        "        [\n",
        "            \"PersonX hand slip, thus it can be seen about PersonX's attribute that PersonX fall\",\n",
        "            \"PersonX fall\"\n",
        "        ],\n",
        "        [\n",
        "            \"PersonX have not hear a car drive up, thus it can be seen about PersonX's attribute that PersonX be startle\",\n",
        "            \"PersonX be startle\",\n",
        "            \"3. mental state\",\n",
        "            \"PersonX have not hear a car drive up, thus it can be seen about PersonX's attribute that PersonX be startle\"\n",
        "        ],\n",
        "        [\n",
        "            \"PersonX walk PersonY through process, thus it can be seen about PersonX's attribute that PersonX be professional\",\n",
        "            \"PersonX be professional\",\n",
        "            \"2. persona\",\n",
        "            \"PersonX walk PersonY through process, thus it can be seen about PersonX's attribute that PersonX be professional\"\n",
        "        ]\n",
        "    ],\n",
        "    \"xIntent\": [\n",
        "        [\n",
        "            \"PersonX ride the light rail, thus it can be seen about PersonX's intention that PersonX do not need a car\",\n",
        "            \"PersonX do not need a car\",\n",
        "            \"PersonX ride the light rail\",\n",
        "            \"PersonX do not need a car\",\n",
        "            \"PersonX ride the light rail\"\n",
        "        ],\n",
        "        [\n",
        "            \"PersonX would at have one person, thus it can be seen about PersonX's intention that PersonX wish\",\n",
        "            \"PersonX wish\",\n",
        "            \"PersonX would at have one person\",\n",
        "            \"PersonX wish\",\n",
        "            \"PersonX would at have one person\",\n",
        "            \"Not determinable\",\n",
        "            \"PersonX would at have one person, thus it can be seen about PersonX's intention that PersonX wish\"\n",
        "        ],\n",
        "        [\n",
        "            \"PersonX can form a opinion, thus it can be seen about PersonX's intention that PersonX have a opinion\",\n",
        "            \"PersonX have a opinion\",\n",
        "            \"PersonX can form a opinion\",\n",
        "            \"PersonX have a opinion\",\n",
        "            \"PersonX can form a opinion\",\n",
        "            \"0\",\n",
        "            \"PersonX can form a opinion, thus it can be seen about PersonX's intention that PersonX have a opinion\"\n",
        "        ]\n",
        "    ],\n",
        "    \"xNeed\": [\n",
        "        [\n",
        "            \"The event PersonX just activate it will not happen unless PersonX use it\",\n",
        "            \"PersonX use it\",\n",
        "            \"PersonX just activate it\",\n",
        "            \"PersonX use it\",\n",
        "            \"PersonX just activate it\"\n",
        "        ],\n",
        "        [\n",
        "            \"The event PersonX go lady at counter will not happen unless PersonX be intolerant\",\n",
        "            \"PersonX be intolerant\",\n",
        "            \"PersonX go lady at counter\",\n",
        "            \"PersonX be intolerant\",\n",
        "            \"PersonX go lady at counter\",\n",
        "            \"Not determinable\",\n",
        "            \"The event PersonX go lady at counter will not happen unless PersonX be intolerant\"\n",
        "        ],\n",
        "        [\n",
        "            \"The event PersonX pay by card will not happen unless PersonX can use PersonX debit card\",\n",
        "            \"PersonX can use PersonX debit card\",\n",
        "            \"PersonX pay by card\",\n",
        "            \"PersonX can use PersonX debit card\",\n",
        "            \"PersonX pay by card\",\n",
        "            \"0\",\n",
        "            \"The event PersonX pay by card will not happen unless PersonX can use PersonX debit card\"\n",
        "        ]\n",
        "    ]\n",
        "}\n",
        "\n",
        "# import pandas as pd\n",
        "# df = pd.read_csv('prompting_data/dev_data_to_tune.csv')\n",
        "# a2atq = {k: v for k,v in zip(df['assertion'], df['assertion_tq'])}\n",
        "# for r in exemplars.keys():\n",
        "#     temp = exemplars[r]\n",
        "#     for i in range(3):\n",
        "#         for j in range(len(temp[i])):\n",
        "#             temp[i][j] = a2atq.get(temp[i][j], temp[i][j])\n",
        "#\n",
        "# import json\n",
        "# print(json.dumps(exemplars, indent=' '*4))\n",
        "\n",
        "exemplars_tq = {\n",
        "    \"xReact\": [\n",
        "        [\n",
        "            \"If PersonX prevent PersonY, as a result, PersonX feels, PersonX never reach out to anyone\",\n",
        "            \"PersonX never reach out to anyone\"\n",
        "        ],\n",
        "        [\n",
        "            \"If PersonX go to sleep on hollow, as a result, PersonX feels, PersonX be tired\",\n",
        "            \"PersonX feel PersonX be tired\",\n",
        "            \"3. mental state\",\n",
        "            \"If PersonX go to sleep on hollow, as a result, PersonX feels, PersonX be tired\"\n",
        "        ],\n",
        "        [\n",
        "            \"If PersonX eat the sub, as a result, PersonX feels, PersonX be full\",\n",
        "            \"PersonX feel PersonX be full\",\n",
        "            \"3. mental state\",\n",
        "            \"If PersonX eat the sub, as a result, PersonX feels, PersonX be full\"\n",
        "        ]\n",
        "    ],\n",
        "    \"oReact\": [\n",
        "        [\n",
        "            \"If PersonX bear PersonY, as a result, PersonY or others feel, PersonY be here\",\n",
        "            \"PersonY be here\"\n",
        "        ],\n",
        "        [\n",
        "            \"If PersonX play ana, as a result, PersonY or others feel, PersonY be fun\",\n",
        "            \"PersonY be fun\",\n",
        "            \"3. mental state\",\n",
        "            \"If PersonX play ana, as a result, PersonY or others feel, PersonY be fun\"\n",
        "        ],\n",
        "        [\n",
        "            \"If PersonX gaze at PersonY, as a result, PersonY or others feel, PersonY blush\",\n",
        "            \"PersonY blush\",\n",
        "            \"3. mental state\",\n",
        "            \"If PersonX gaze at PersonY, as a result, PersonY or others feel, PersonY blush\"\n",
        "        ]\n",
        "    ],\n",
        "    \"xAttr\": [\n",
        "        [\n",
        "            \"If PersonX hand slip, PersonX is seen as, PersonX fall\",\n",
        "            \"PersonX fall\"\n",
        "        ],\n",
        "        [\n",
        "            \"If PersonX have not hear a car drive up, PersonX is seen as, PersonX be startle\",\n",
        "            \"PersonX be startle\",\n",
        "            \"3. mental state\",\n",
        "            \"If PersonX have not hear a car drive up, PersonX is seen as, PersonX be startle\"\n",
        "        ],\n",
        "        [\n",
        "            \"If PersonX walk PersonY through process, PersonX is seen as, PersonX be professional\",\n",
        "            \"PersonX be professional\",\n",
        "            \"2. persona\",\n",
        "            \"If PersonX walk PersonY through process, PersonX is seen as, PersonX be professional\"\n",
        "        ]\n",
        "    ],\n",
        "    \"xIntent\": [\n",
        "        [\n",
        "            \"If PersonX ride the light rail, because PersonX wanted, PersonX do not need a car\",\n",
        "            \"PersonX do not need a car\",\n",
        "            \"PersonX ride the light rail\",\n",
        "            \"PersonX do not need a car\",\n",
        "            \"PersonX ride the light rail\"\n",
        "        ],\n",
        "        [\n",
        "            \"If PersonX would at have one person, because PersonX wanted, PersonX wish\",\n",
        "            \"PersonX wish\",\n",
        "            \"PersonX would at have one person\",\n",
        "            \"PersonX wish\",\n",
        "            \"PersonX would at have one person\",\n",
        "            \"Not determinable\",\n",
        "            \"If PersonX would at have one person, because PersonX wanted, PersonX wish\"\n",
        "        ],\n",
        "        [\n",
        "            \"If PersonX can form a opinion, because PersonX wanted, PersonX have a opinion\",\n",
        "            \"PersonX have a opinion\",\n",
        "            \"PersonX can form a opinion\",\n",
        "            \"PersonX have a opinion\",\n",
        "            \"PersonX can form a opinion\",\n",
        "            \"0\",\n",
        "            \"If PersonX can form a opinion, because PersonX wanted, PersonX have a opinion\"\n",
        "        ]\n",
        "    ],\n",
        "    \"xNeed\": [\n",
        "        [\n",
        "            \"If PersonX just activate it, but before, PersonX needed, PersonX use it\",\n",
        "            \"PersonX use it\",\n",
        "            \"PersonX just activate it\",\n",
        "            \"PersonX use it\",\n",
        "            \"PersonX just activate it\"\n",
        "        ],\n",
        "        [\n",
        "            \"If PersonX go lady at counter, but before, PersonX needed, PersonX be intolerant\",\n",
        "            \"PersonX be intolerant\",\n",
        "            \"PersonX go lady at counter\",\n",
        "            \"PersonX be intolerant\",\n",
        "            \"PersonX go lady at counter\",\n",
        "            \"Not determinable\",\n",
        "            \"If PersonX go lady at counter, but before, PersonX needed, PersonX be intolerant\"\n",
        "        ],\n",
        "        [\n",
        "            \"If PersonX pay by card, but before, PersonX needed, PersonX can use PersonX debit card\",\n",
        "            \"PersonX can use PersonX debit card\",\n",
        "            \"PersonX pay by card\",\n",
        "            \"PersonX can use PersonX debit card\",\n",
        "            \"PersonX pay by card\",\n",
        "            \"0\",\n",
        "            \"If PersonX pay by card, but before, PersonX needed, PersonX can use PersonX debit card\"\n",
        "        ]\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "0aZx9iaSNeAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "least2most_head_prompt = \"For each statement below, please answer several questions to reach the final conclusion if the statement is commonsense.\" + \\\n",
        "\" Whenever your answer of a question is No, please conclude that the statement is not commonsense. Otherwise, please conclude that the statement is commonsense.\\n\"\n",
        "\n",
        "# assertion + prompt1\n",
        "question_lists = {\n",
        "    'typing': [\"\"\"\n",
        "Statement: {}\n",
        "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause '{}' express. Answer the choice only.\n",
        "A: \"\"\", \"\"\"\n",
        "Q: Is the above answer different from option 1. event/activity?\n",
        "A: \"\"\", ''],\n",
        "    'temporal': [\"\"\"\n",
        "Statement: {}\n",
        "Q: Which one of the following two related statements is more plausible:\n",
        "0. {} before {}\n",
        "1. {} after {}\n",
        "Answer 0 or 1 only.\n",
        "A: \"\"\", \"\"\"\n",
        "Q: Is the above answer different from 1?\n",
        "A: \"\"\", '']\n",
        "}\n",
        "question_lists_tq = copy.deepcopy(question_lists)\n",
        "\n",
        "question_lists['typing'][-1] = question_lists['temporal'][-1] = \"\"\"\n",
        "Q: Is the statement \"{}\" likely to occur?\n",
        "A: \"\"\"\n",
        "question_lists_tq['typing'][-1] = question_lists_tq['temporal'][-1] = \"\"\"\n",
        "Q: Is the statement \"{}\" plausible?\n",
        "A: \"\"\"\n",
        "\n",
        "\n",
        "def exemplar_prompt(e, question_lists):\n",
        "    if e == 'typing':\n",
        "        return  question_lists['typing'][0] + '1. event/activity' + \\\n",
        "                question_lists['typing'][1] + 'No. Thus, the statement is not commonsense\\n' + \\\n",
        "                question_lists['typing'][0] + '{}' + \\\n",
        "                question_lists['typing'][1] + 'Yes' + \\\n",
        "                question_lists['typing'][2] + 'No. Thus, the statement is not commonsense\\n' + \\\n",
        "                question_lists['typing'][0] + '{}' + \\\n",
        "                question_lists['typing'][1] + 'Yes' + \\\n",
        "                question_lists['typing'][2] + 'Yes. Thus, the statement is commonsense\\n'\n",
        "    elif e == 'temporal':\n",
        "        return  question_lists['temporal'][0] + '1' + \\\n",
        "                question_lists['temporal'][1] + 'No. Thus, the statement is not commonsense\\n' + \\\n",
        "                question_lists['temporal'][0] + '{}' + \\\n",
        "                question_lists['temporal'][1] + 'Yes' + \\\n",
        "                question_lists['temporal'][2] + 'No. Thus, the statement is not commonsense\\n' + \\\n",
        "                question_lists['temporal'][0] + '{}' + \\\n",
        "                question_lists['temporal'][1] + 'Yes' + \\\n",
        "                question_lists['temporal'][2] + 'Yes. Thus, the statement is commonsense\\n'\n",
        "\n",
        "\n",
        "e = 'typing'\n",
        "r = 'xReact'\n",
        "# least2most_prompt1\n",
        "print(exemplar_prompt(e, question_lists_tq).format(*[c for instance in exemplars_tq[r] for c in instance]))\n",
        "# least2most_prompt2\n",
        "print(exemplar_prompt(e, question_lists_tq).format(*[c for instance in exemplars[r] for c in instance]))\n",
        "# least2most_prompt3\n",
        "print(exemplar_prompt(e, question_lists).format(*[c for instance in exemplars[r] for c in instance]))\n"
      ],
      "metadata": {
        "id": "kpQk1LrZhsoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06c578e-0438-4373-d9a8-393ed8d59383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Statement: If PersonX prevent PersonY, as a result, PersonX feels, PersonX never reach out to anyone\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX never reach out to anyone' express. Answer the choice only.\n",
            "A: 1. event/activity\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: No. Thus, the statement is not commonsense\n",
            "\n",
            "Statement: If PersonX go to sleep on hollow, as a result, PersonX feels, PersonX be tired\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX feel PersonX be tired' express. Answer the choice only.\n",
            "A: 3. mental state\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: Yes\n",
            "Q: Is the statement \"If PersonX go to sleep on hollow, as a result, PersonX feels, PersonX be tired\" plausible?\n",
            "A: No. Thus, the statement is not commonsense\n",
            "\n",
            "Statement: If PersonX eat the sub, as a result, PersonX feels, PersonX be full\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX feel PersonX be full' express. Answer the choice only.\n",
            "A: 3. mental state\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: Yes\n",
            "Q: Is the statement \"If PersonX eat the sub, as a result, PersonX feels, PersonX be full\" plausible?\n",
            "A: Yes. Thus, the statement is commonsense\n",
            "\n",
            "\n",
            "Statement: PersonX prevent PersonY, thus as a result on PersonX's emotion, PersonX never reach out to anyone\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX never reach out to anyone' express. Answer the choice only.\n",
            "A: 1. event/activity\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: No. Thus, the statement is not commonsense\n",
            "\n",
            "Statement: PersonX go to sleep on hollow, thus as a result on PersonX's emotion, PersonX feel PersonX be tired\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX feel PersonX be tired' express. Answer the choice only.\n",
            "A: 3. mental state\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: Yes\n",
            "Q: Is the statement \"PersonX go to sleep on hollow, thus as a result on PersonX's emotion, PersonX feel PersonX be tired\" plausible?\n",
            "A: No. Thus, the statement is not commonsense\n",
            "\n",
            "Statement: PersonX eat the sub, thus as a result on PersonX's emotion, PersonX feel PersonX be full\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX feel PersonX be full' express. Answer the choice only.\n",
            "A: 3. mental state\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: Yes\n",
            "Q: Is the statement \"PersonX eat the sub, thus as a result on PersonX's emotion, PersonX feel PersonX be full\" plausible?\n",
            "A: Yes. Thus, the statement is commonsense\n",
            "\n",
            "\n",
            "Statement: PersonX prevent PersonY, thus as a result on PersonX's emotion, PersonX never reach out to anyone\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX never reach out to anyone' express. Answer the choice only.\n",
            "A: 1. event/activity\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: No. Thus, the statement is not commonsense\n",
            "\n",
            "Statement: PersonX go to sleep on hollow, thus as a result on PersonX's emotion, PersonX feel PersonX be tired\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX feel PersonX be tired' express. Answer the choice only.\n",
            "A: 3. mental state\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: Yes\n",
            "Q: Is the statement \"PersonX go to sleep on hollow, thus as a result on PersonX's emotion, PersonX feel PersonX be tired\" likely to occur?\n",
            "A: No. Thus, the statement is not commonsense\n",
            "\n",
            "Statement: PersonX eat the sub, thus as a result on PersonX's emotion, PersonX feel PersonX be full\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX feel PersonX be full' express. Answer the choice only.\n",
            "A: 3. mental state\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: Yes\n",
            "Q: Is the statement \"PersonX eat the sub, thus as a result on PersonX's emotion, PersonX feel PersonX be full\" likely to occur?\n",
            "A: Yes. Thus, the statement is commonsense\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['prediction'] = 0\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt + exemplar_prompt(e, question_lists).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in data_df.index[data_df['relation'] == r]:\n",
        "        tail = data_df['tail'][i]\n",
        "        head = data_df['head'][i]\n",
        "        assertion = data_df['assertion'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = chatgpt(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = chatgpt(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        # if 'no' in answer.lower():\n",
        "        #     print(instance_prompt)\n",
        "        #     continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists[e][2].format(assertion)\n",
        "        answer = chatgpt(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer:\n",
        "            data_df['prediction'][i] = 1\n",
        "\n",
        "        print(instance_prompt)\n",
        "        break"
      ],
      "metadata": {
        "id": "szDRaiQ3IDP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# no exemplar\n",
        "data_df['prediction'] = 0\n",
        "func = chatgpt\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt #+ exemplar_prompt(e, question_lists).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in data_df.index[data_df['relation'] == r]:\n",
        "        tail = data_df['tail'][i]\n",
        "        head = data_df['head'][i]\n",
        "        assertion = data_df['assertion'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        # if 'no' in answer.lower():\n",
        "        #     print(instance_prompt)\n",
        "        #     continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists[e][2].format(assertion)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer:\n",
        "            data_df['prediction'][i] = 1\n",
        "\n",
        "        print(instance_prompt)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLDd57QdfP4I",
        "outputId": "5f1d8d5c-4ef8-42ae-cbd7-faf85cd2ebe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Statement: PersonX pull off, thus as a result on PersonX's emotion, PersonX wait\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX wait' express. Answer the choice only.\n",
            "A: 3. mental state\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: Yes\n",
            "Q: Is the statement \"PersonX pull off, thus as a result on PersonX's emotion, PersonX wait\" likely to occur?\n",
            "A: No\n",
            "\n",
            "Statement: PersonX bear PersonY, thus as a result on PersonY's emotion, PersonY feel PersonY be here\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonY be here' express. Answer the choice only.\n",
            "A: 3. mental state\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: Yes,\n",
            "Q: Is the statement \"PersonX bear PersonY, thus as a result on PersonY's emotion, PersonY feel PersonY be here\" likely to occur?\n",
            "A: No, the statement does not make sense and is\n",
            "\n",
            "Statement: PersonX baby brother die, thus it can be seen about PersonX's attribute that PersonX be babysit\n",
            "Q: Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX be babysit' express. Answer the choice only.\n",
            "A: persona\n",
            "\n",
            "Q: Does\n",
            "Q: Is the above answer different from option 1. event/activity?\n",
            "A: Yes\n",
            "Q: Is the statement \"PersonX baby brother die, thus it can be seen about PersonX's attribute that PersonX be babysit\" likely to occur?\n",
            "A: No\n",
            "\n",
            "Statement: PersonX believe thing, thus it can be seen about PersonX's intention that PersonX belief be true\n",
            "Q: Which one of the following two related statements is more plausible:\n",
            "0. PersonX belief be true before PersonX believe thing\n",
            "1. PersonX belief be true after PersonX believe thing\n",
            "Answer 0 or 1 only.\n",
            "A: 1 (not commonsense\n",
            "Q: Is the above answer different from 1?\n",
            "A: No.\n",
            "Q: Is the statement \"PersonX believe thing, thus it can be seen about PersonX's intention that PersonX belief be true\" likely to occur?\n",
            "A: No.\n",
            "\n",
            "Statement: The event PersonX get even happier will not happen unless PersonX be happy\n",
            "Q: Which one of the following two related statements is more plausible:\n",
            "0. PersonX be happy before PersonX get even happier\n",
            "1. PersonX be happy after PersonX get even happier\n",
            "Answer 0 or 1 only.\n",
            "A: 0 (commonsense)\n",
            "Q: Is the above answer different from 1?\n",
            "A: Yes (\n",
            "Q: Is the statement \"The event PersonX get even happier will not happen unless PersonX be happy\" likely to occur?\n",
            "A: Yes (commonsense)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p92IoO9PJRY"
      },
      "source": [
        "## text-davinci-003\n",
        "\n",
        "Test if text-davinci-003 is a strict model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxbjXwfZQr_5"
      },
      "source": [
        "#### direct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WUPvL7hPL7v"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "bs = 20\n",
        "\n",
        "for i in range((len(data)-1)//bs + 1):\n",
        "    p = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=[judge_prompt_1.format(x) for x in data[i*bs:(i+1)*bs]],\n",
        "        max_tokens=20,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(p['choices'])\n",
        "    predictions.extend(p['choices'])\n",
        "\n",
        "# omg, only 5 True!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cYmJt7uR4UF"
      },
      "outputs": [],
      "source": [
        "judge_prompt_4 = \"Judge the following statements if they are likely to occur, only list question number and answer 'True' or 'False' for each:\\n{}\"\n",
        "bs = 20\n",
        "\n",
        "for i in range(1, (len(data)-1)//bs + 1):\n",
        "    pred = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=judge_prompt_4.format('\\n'.join([f'{j}. {x}' for j, x in enumerate(data[i*bs:(i+1)*bs])])),\n",
        "        max_tokens=500,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(pred)\n",
        "\n",
        "# not so bad 0.7 auc, but :))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_F7y-zR5P3d"
      },
      "source": [
        "batch judge is .. better than individual judge, as individual judge is too strict (only 5 out of 102 are True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CoT"
      ],
      "metadata": {
        "id": "4kKPScxIxMJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=[zeroshotCoT_prompt_tq.format(data_df['assertion'][s])\n",
        "        for s in [0, 4] # range(i*bs, min((i+1)*bs, len(data_df)))\n",
        "    ],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "print(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TttkPm-gxh8K",
        "outputId": "8f765389-919e-4a3e-8cfa-59b0f80934ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----False. Ejaculation does not necessarily mean that a person wants to experience an orgasm. It is possible for a person to ejaculate without wanting or experiencing an orgasm.\n",
            "----True. Enlisting in the military is a sign of wanting to fight for one's country.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=[zeroshotCoT_prompt1.format(data_df['assertion_tq'][s])\n",
        "        for s in [0, 4] # range(i*bs, min((i+1)*bs, len(data_df)))\n",
        "    ],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "print(temp)"
      ],
      "metadata": {
        "id": "iQHYZKIg8ma6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d3cdb14-6fdf-41ef-d405-11ffdf2ff4ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----True. Ejaculation often leads to orgasm, so it is likely to occur.\n",
            "----Step 1: PersonX enlists in the military.--True----Step 2: As a result, PersonX wants to fight for their country.--True----Conclusion: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=[zeroshotCoT_prompt1.format(data_df['assertion'][s])\n",
        "        for s in range(20) # range(i*bs, min((i+1)*bs, len(data_df)))\n",
        "    ],\n",
        "    max_tokens=50,\n",
        "    temperature=0\n",
        ")\n",
        "temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "print(temp)"
      ],
      "metadata": {
        "id": "VsAZSSTo8mzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1670709-e0d4-499c-c7de-739dcfc3ca8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----False. Ejaculation does not necessarily mean that a person wants to experience an orgasm. It is possible for a person to ejaculate without wanting or experiencing an orgasm.\n",
            "----False. PersonX may go to the fridge for many reasons, such as to get a snack or a drink of water.\n",
            "----False. Working tirelessly does not necessarily mean that a person wants to die.\n",
            "----True\n",
            "----Step 1: Is PersonX enlisting?--Yes----Step 2: Does PersonX want to fight for their country?--Yes----Conclusion: True\n",
            "----False. It is unlikely that a person falling into a pool would lead to their mother enrolling them in school.\n",
            "----False. It is unlikely that a person would want to put down toilet paper in the toilet after they have already defecated.\n",
            "----False. It is unlikely that a person would come and want to be paranoid.\n",
            "----False. This statement does not make sense.\n",
            "----False. It is unlikely that PersonX would give PersonY a substantial amount if they knew PersonY could not pay it back.\n",
            "----True\n",
            "----False. Having a cavity does not necessarily mean that a person will have a toothache. A person may have a cavity and not experience any pain or discomfort.\n",
            "----False. Wiring someone money does not necessarily mean that the sender wants to get a telegram.\n",
            "----False. It is not likely that PersonY would want to escape simply because PersonX did.\n",
            "----Step 1: Is PersonX currently working from home?--True----Step 2: Does PersonY want to be a remote worker?--True----Conclusion: True\n",
            "----False. PersonX's loan success does not necessarily mean that PersonY will want to prefer it.\n",
            "----False. It is unlikely that PersonY would want PersonX not to look at them.\n",
            "----False. This statement does not make sense.\n",
            "----False. Leaving food does not necessarily result in being asked.\n",
            "----False. It is highly unlikely that a person collapsing in a courthouse would cause their body to reject a transplant liver.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REMY_BReQqAF"
      },
      "source": [
        "#### abstraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK9P4drEMqfL"
      },
      "outputs": [],
      "source": [
        "pred = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=old_conceptualization_prompt.format(data[34], 'emotion'),\n",
        "    max_tokens=100,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWmXazZk0bxt"
      },
      "outputs": [],
      "source": [
        "def query_w_abstraction(idx, conceptualization_prompt=conceptualization_prompt):\n",
        "    if type(idx) != list:\n",
        "        idx = [idx]\n",
        "    pred = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=[conceptualization_prompt.format(\n",
        "            data_df['assertion'][i],\n",
        "            data_df['dim'][i],\n",
        "            data_df['head'][i],\n",
        "            data_df['tail'][i]\n",
        "        ) for i in idx],\n",
        "        max_tokens=100,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    print(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YQFLMPe-Pfy"
      },
      "source": [
        "##### all dev instances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS91yjS0-jpL"
      },
      "source": [
        "same-prompt batch (less effective than individual)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZoH47Da-k-U"
      },
      "source": [
        "individual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKLpxLuMLENt"
      },
      "source": [
        "5 exemplars: actually better @@"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCa51SG1KvTu"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "bs = 10\n",
        "\n",
        "for b in range((len(data)-1)//bs + 1):\n",
        "    p = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=[conceptualization_prompt_5exemplars.format(\n",
        "            data_df['assertion'][i],\n",
        "            data_df['dim'][i],\n",
        "            data_df['head'][i],\n",
        "            data_df['tail'][i])\n",
        "            for i in range(b*bs, min((b+1)*bs, len(data_df)))\n",
        "        ],\n",
        "        max_tokens=100,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(p['choices'])\n",
        "    predictions.extend(p['choices'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KaY1BvzLBhF"
      },
      "outputs": [],
      "source": [
        "print('\\n'.join('##' + str(x['index']) + x['text'] for x in predictions))\n",
        "# print('\\n'.join(str(x['index']) + x['text'].rsplit('\\n',1)[1][1:] for x in predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH5dtjGMLCEV"
      },
      "source": [
        "two exemplars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6filWyT-mWT"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "bs = 10\n",
        "\n",
        "for b in range((len(data)-1)//bs + 1):\n",
        "    p = openai.Completion.create(\n",
        "        model=\"text-davinci-003\",\n",
        "        prompt=[conceptualization_prompt.format(\n",
        "            data_df['assertion'][i],\n",
        "            data_df['dim'][i],\n",
        "            data_df['head'][i],\n",
        "            data_df['tail'][i])\n",
        "            for i in range(b*bs, min((b+1)*bs, len(data_df)))\n",
        "        ],\n",
        "        max_tokens=100,\n",
        "        temperature=0\n",
        "    )\n",
        "    print(p['choices'])\n",
        "    predictions.extend(p['choices'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VoVXHOnJU1g"
      },
      "outputs": [],
      "source": [
        "print('\\n'.join('##' + str(x['index']) + x['text'] for x in predictions))\n",
        "# print('\\n'.join(str(x['index']) + x['text'].rsplit('\\n',1)[1][1:] for x in predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwMMNBmD-MLz"
      },
      "source": [
        "##### draft experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdYLqctW-ghC"
      },
      "source": [
        "batch\n",
        "\n",
        "- run slower\n",
        "- yield longer answer (repeat instance) -> cost more\n",
        "- less accurate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zrD9_P9D0xT"
      },
      "outputs": [],
      "source": [
        "samples = [conceptualization_prompt_batch_input_tail.format(\n",
        "            data_df['assertion'][i],\n",
        "            data_df['dim'][i],\n",
        "            data_df['head'][i],\n",
        "            data_df['tail'][i]\n",
        "            ) for i in [34, 38, 68, 69]]\n",
        "prompt = conceptualization_prompt_batch_input + '\\n'.join(samples)\n",
        "    # '\\n'.join([f'{j}. {x}' for j, x in enumerate(samples)])\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af88pCbS-eck"
      },
      "outputs": [],
      "source": [
        "pred = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    max_tokens=600,\n",
        "    temperature=0\n",
        ")\n",
        "print(pred)\n",
        "print(pred['choices'][0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFMda7qC-fE9"
      },
      "source": [
        "individual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt5CiNOt3d_T"
      },
      "outputs": [],
      "source": [
        "for i in [34, 38, 68, 69]:\n",
        "    query_w_abstraction(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EeYbLlE4jA3"
      },
      "source": [
        "Very unpredictable! more exemplar, worse judgment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiXq22qpj1A0"
      },
      "outputs": [],
      "source": [
        "# use conceptualization_prompt_5exemplars\n",
        "\n",
        "for i in [34, 38, 68, 69]:\n",
        "    query_w_abstraction(i, conceptualization_prompt_5exemplars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HQTpwmE0nMF"
      },
      "outputs": [],
      "source": [
        "query_w_abstraction(69)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY0l3OQo04Yc"
      },
      "source": [
        "Insight: text-davinci-003 judging samples individually seems to be strict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COVtajqDWU8Y"
      },
      "source": [
        "### code-davinci-003\n",
        "\n",
        "OMG, no code-davinci-003, while code-davinci-002 is deprecated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AODugIRIkwKp"
      },
      "source": [
        "## OpenAI official request batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxyDSzG1kIsR"
      },
      "outputs": [],
      "source": [
        "prompt = judge_prompt_1.format(data[0])\n",
        "gen_chatgpt = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    max_tokens=7, temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFl8F7dDqBGB"
      },
      "outputs": [],
      "source": [
        "gen_chatgpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDOIuMfsqDWv"
      },
      "outputs": [],
      "source": [
        "gen_chatgpt = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo-0301\",\n",
        "    messages=[[{\"role\": \"user\", \"content\": judge_prompt_1.format(s)}] for s in data[1:4]],\n",
        "    max_tokens=4, temperature=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF_I2ZUTrttF"
      },
      "outputs": [],
      "source": [
        "openai.ChatCompletion.create.__doc__\n",
        "# ChatCompletion doesn't support batching yet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1IY65nBsgaE"
      },
      "outputs": [],
      "source": [
        "num_stories = 2\n",
        "prompts = [\"Once upon a time,\"] * num_stories\n",
        "\n",
        "# batched example, with 10 story completions per request\n",
        "response = openai.Completion.create(\n",
        "    model=\"curie\",\n",
        "    prompt=prompts,\n",
        "    max_tokens=20,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "371PIOZ5wTaB"
      },
      "outputs": [],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va_Ss0NasHG2"
      },
      "source": [
        "## [IMPORTANT] Run on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZFSHgiUsWFG",
        "outputId": "0973ee6f-285c-4356-d2ee-593182e57b36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['head', 'relation', 'tail', 'class', 'split', 'label', 'dim',\n",
            "       'assertion', 'assertion_tq', 'naive1', 'naive2', 'naive3'],\n",
            "      dtype='object') 979 979\n"
          ]
        }
      ],
      "source": [
        "tst_data_df = pd.read_csv('data/tst_data_to_eval.csv')\n",
        "tst_data = tst_data_df['assertion'].tolist()\n",
        "\n",
        "tst_kate_exemplars = pd.read_csv('data/tst_kate_exemplars_top5.csv')\n",
        "tst_kate_exemplars_tq = pd.read_csv('data/tst_kate_exemplars_tq_top5.csv')\n",
        "tst_kate_same_relation_exemplars = pd.read_csv('data/tst_kate_same_relation_exemplars_top5.csv')\n",
        "tst_kate_same_relation_exemplars_tq = pd.read_csv('data/tst_kate_same_relation_exemplars_tq_top5.csv')\n",
        "tst_random_exemplars = pd.read_csv('data/tst_random_5exemplars.csv')\n",
        "\n",
        "print(tst_data_df.columns, len(tst_data), len(tst_kate_exemplars))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for df in [tst_kate_exemplars,\n",
        "  tst_kate_exemplars_tq,\n",
        "  tst_kate_same_relation_exemplars,\n",
        "  tst_kate_same_relation_exemplars_tq,\n",
        "  tst_random_exemplars]:\n",
        "  print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zsnxC6p_5wL",
        "outputId": "1c49ba3c-5057-491d-f78f-d87d74febb0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['0', '0_label', '1', '1_label', '2', '2_label', '3', '3_label', '4',\n",
            "       '4_label'],\n",
            "      dtype='object')\n",
            "Index(['0_tq', '0_tq_label', '1_tq', '1_tq_label', '2_tq', '2_tq_label',\n",
            "       '3_tq', '3_tq_label', '4_tq', '4_tq_label'],\n",
            "      dtype='object')\n",
            "Index(['0', '0_label', '1', '1_label', '2', '2_label', '3', '3_label', '4',\n",
            "       '4_label'],\n",
            "      dtype='object')\n",
            "Index(['0_tq', '0_tq_label', '1_tq', '1_tq_label', '2_tq', '2_tq_label',\n",
            "       '3_tq', '3_tq_label', '4_tq', '4_tq_label'],\n",
            "      dtype='object')\n",
            "Index(['0', '0_label', '1', '1_label', '2', '2_label', '3', '3_label', '4',\n",
            "       '4_label', '0_tq', '0_tq_label', '1_tq', '1_tq_label', '2_tq',\n",
            "       '2_tq_label', '3_tq', '3_tq_label', '4_tq', '4_tq_label', '5',\n",
            "       '5_label', '6', '6_label', '7', '7_label', '8', '8_label', '9',\n",
            "       '9_label'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXj2ipaHjGXp"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "# output = open('data/output.txt', 'a+')\n",
        "# output.close()\n",
        "# openai.api_key ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZfvbikElgz3"
      },
      "outputs": [],
      "source": [
        "bs = 5 #@param\n",
        "total = (len(tst_data_df)-1)//bs+1\n",
        "timeout_succeed = 1 #@param\n",
        "timeout_failure = 10 #@param"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def chatgpt_loop(customize_prompt, max_tokens, filename_suffix, resume_at_step=0, end_at_step=total):\n",
        "    for i in range(resume_at_step, end_at_step):\n",
        "        while True:\n",
        "            try:\n",
        "                print(i, '/', total)\n",
        "                pred = await dispatch_openai_requests(\n",
        "                        messages_list = [[{\"role\": \"user\", \"content\": customize_prompt(s)}]\n",
        "                            for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))],\n",
        "                        max_tokens=max_tokens,\n",
        "                    )\n",
        "                temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "                print(temp)\n",
        "                output = open(f'data/tst_gpt3.5-turbo_{filename_suffix}.txt', 'a+')\n",
        "                output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "                output.close()\n",
        "                predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "                time.sleep(timeout_succeed)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "0g_NuTAE1ytb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def davinci_loop(customize_prompt, max_tokens, filename_suffix, resume_at_step=0, end_at_step=total):\n",
        "    for i in range(resume_at_step, end_at_step):\n",
        "        while True:\n",
        "            try:\n",
        "                print(i, '/', total)\n",
        "                p = openai.Completion.create(\n",
        "                    model=\"text-davinci-003\",\n",
        "                    prompt=[customize_prompt(s)\n",
        "                        for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=0\n",
        "                )\n",
        "                temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "                print(temp)\n",
        "                output = open(f'data/tst_davinci_{filename_suffix}.txt', 'a+')\n",
        "                output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "                output.close()\n",
        "                predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "                time.sleep(timeout_succeed)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "cKa026WYx7T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYjOUDMrUY3F"
      },
      "source": [
        "### gpt3.5-turbo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "naive"
      ],
      "metadata": {
        "id": "T2FdTjs6-O-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_prompt1 = lambda s:judge_prompt_0.format(tst_data_df['assertion_tq'][s])\n",
        "print(naive_prompt1(0))\n",
        "\n",
        "await chatgpt_loop(naive_prompt1, max_tokens=2, filename_suffix='naive_prompt1', resume_at_step=0)"
      ],
      "metadata": {
        "id": "t32h_qnB5S_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_prompt2 = lambda s:judge_prompt_0.format(tst_data_df['assertion'][s])\n",
        "print(naive_prompt2(0))\n",
        "\n",
        "await chatgpt_loop(naive_prompt2, max_tokens=2, filename_suffix='naive_prompt2', resume_at_step=0)"
      ],
      "metadata": {
        "id": "1kWaZupL5m35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_prompt3 = lambda s:judge_prompt_1.format(tst_data_df['assertion'][s])\n",
        "print(naive_prompt3(0))\n",
        "\n",
        "await chatgpt_loop(naive_prompt3, max_tokens=2, filename_suffix='naive_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "6eC-ejxu5nR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "moe ~ system1"
      ],
      "metadata": {
        "id": "J8xULX-7-QzQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAb-C7sxGyQV"
      },
      "outputs": [],
      "source": [
        "# run MoE first\n",
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q1_prompt1.format(s)}] for s in tst_data_df['tail'][i*bs:(i+1)*bs]],\n",
        "                    max_tokens=4,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/output.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkuMugYOHSX6"
      },
      "outputs": [],
      "source": [
        "# run MoE q2head\n",
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q2_prompt1.format(s)}] for s in tst_data_df['head'][i*bs:(i+1)*bs]],\n",
        "                    max_tokens=4,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_moe_q2head_prompt1.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAPA-7upHa2J"
      },
      "outputs": [],
      "source": [
        "# run MoE q2tail\n",
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q2_prompt1.format(s)}] for s in tst_data_df['tail'][i*bs:(i+1)*bs]],\n",
        "                    max_tokens=4,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_moe_q2tail_prompt1.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "moe_q2_prompt2_fn = lambda s:moe_q2_prompt2.format(tst_data_df['head'][s], tst_data_df['tail'][s])\n",
        "print(moe_q2_prompt2_fn(0))\n",
        "\n",
        "await chatgpt_loop(moe_q2_prompt2_fn, max_tokens=2, filename_suffix='moe_q2_prompt2', resume_at_step=0)"
      ],
      "metadata": {
        "id": "0VZ276kjhHFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMGnFnURPgzd"
      },
      "outputs": [],
      "source": [
        "idx_q3 = tst_data_df['relation'].isin(['xIntent', 'xNeed'])\n",
        "temp_df = tst_data_df[idx_q3]\n",
        "temp_df.reset_index(inplace=True)\n",
        "total = (len(temp_df)-1)//bs+1\n",
        "temp_df\n",
        "\n",
        "for b in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(b, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q3_prompt3.format(temp_df['tail'][i], temp_df['head'][i])}] \\\n",
        "                        for i in range(b*bs, min((b+1)*bs, len(temp_df)))],\n",
        "                    max_tokens=4,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_moe_q3_prompt3.txt', 'a+')\n",
        "            output.write(f'{b}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69zL6C3ol00o"
      },
      "outputs": [],
      "source": [
        "for b in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(b, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q3_prompt3.format(tst_data_df['tail'][i], tst_data_df['head'][i])}] \\\n",
        "                        for i in range(b*bs, min((b+1)*bs, len(tst_data_df)))],\n",
        "                    max_tokens=2,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_moe_q3_prompt3.txt', 'a+')\n",
        "            output.write(f'{b}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = tst_data_df[tst_data_df['relation'].isin('xReact,oReact,xAttr'.split(','))]\n",
        "temp_df.reset_index(inplace=True)\n",
        "total = (len(temp_df)-1)//bs+1\n",
        "temp_df\n",
        "\n",
        "for b in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(b, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q1_prompt2.format(temp_df['tail'][i])}] \\\n",
        "                        for i in range(b*bs, min((b+1)*bs, len(temp_df)))],\n",
        "                    max_tokens=4,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_moe_q1_prompt2.txt', 'a+')\n",
        "            output.write(f'{b}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "Y3wnMOHj_1CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = tst_data_df[tst_data_df['relation'].isin(['xIntent', 'xNeed'])]\n",
        "temp_df.reset_index(inplace=True)\n",
        "total = (len(temp_df)-1)//bs+1\n",
        "temp_df\n",
        "\n",
        "for b in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(b, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q3_prompt1.format(temp_df['tail'][i], temp_df['head'][i])}] \\\n",
        "                        for i in range(b*bs, min((b+1)*bs, len(temp_df)))],\n",
        "                    max_tokens=4,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_moe_q3_prompt1.txt', 'a+')\n",
        "            output.write(f'{b}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "BRUw0yUK-jF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "zeroshotCoT"
      ],
      "metadata": {
        "id": "nlXqKZGa-VXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": zeroshotCoT_prompt_tq.format(tst_data_df['assertion'][s])}]\n",
        "                        for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))],\n",
        "                    max_tokens=150,\n",
        "                )\n",
        "            temp = '\\n--'.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_zeroshot_prompt1.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "Nx7rlkL_ocbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": zeroshotCoT_prompt1.format(tst_data_df['assertion_tq'][s])}]\n",
        "                        for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))],\n",
        "                    max_tokens=150,\n",
        "                )\n",
        "            temp = '\\n--'.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_zeroshot_prompt2.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "VyHavPLk-TQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": zeroshotCoT_prompt1.format(tst_data_df['assertion'][s])}]\n",
        "                        for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))],\n",
        "                    max_tokens=150,\n",
        "                )\n",
        "            temp = '\\n--'.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_zeroshot_prompt3.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "M7cOh1pr-i2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kate top1"
      ],
      "metadata": {
        "id": "FEmoFNbTUHLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kate_prompt1\n",
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                     messages_list = [\n",
        "                        [{\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": kate_prompt_0.format('\\n\\n'.join([\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0_tq'][s], 'Yes'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0_tq'][s], 'No'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion_tq'][s], '')]))\n",
        "                        }]\n",
        "                        for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                    ],\n",
        "                    max_tokens=2,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_kate_top1_prompt1.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "cdP03uPEWGou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kate_prompt2\n",
        "for i in range(0 total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                     messages_list = [\n",
        "                        [{\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": kate_prompt_0.format('\\n\\n'.join([\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0'][s], 'Yes'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0'][s], 'No'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]))\n",
        "                        }]\n",
        "                        for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                    ],\n",
        "                    max_tokens=2,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_kate_top1_prompt2.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "L4LKddOEQ34N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kate_prompt3\n",
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                     messages_list = [\n",
        "                        [{\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": kate_prompt_1.format('\\n\\n'.join([\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0'][s], 'True'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0'][s], 'False'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]))\n",
        "                        }]\n",
        "                        for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                    ],\n",
        "                    max_tokens=2,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/tst_gpt3.5-turbo_kate_top1_prompt3.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "JtNsqF4pT2_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kate top5"
      ],
      "metadata": {
        "id": "XQAWLvpImJit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kate_top5_prompt1 = lambda s:kate_prompt_0.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars[f'kate_{p}_{_}_tq'][s], a) \\\n",
        "        for _ in range(5) for p, a in [('pos', 'Yes'), ('neg', 'No')]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion_tq'][s], '')]\n",
        "))\n",
        "print(kate_top5_prompt1(0))\n",
        "\n",
        "# await chatgpt_loop(kate_top5_prompt1, max_tokens=2, filename_suffix='kate_top5_prompt1', resume_at_step=0)"
      ],
      "metadata": {
        "id": "hnQRfpMf3RRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kate_top5_prompt2 = lambda s:kate_prompt_0.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars[f'kate_{p}_{_}'][s], a) \\\n",
        "        for _ in range(5) for p, a in [('pos', 'Yes'), ('neg', 'No')]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_top5_prompt2(0))\n",
        "\n",
        "# await chatgpt_loop(kate_top5_prompt2, max_tokens=2, filename_suffix='kate_top5_prompt2', resume_at_step=0)"
      ],
      "metadata": {
        "id": "GWVUDDjb7-8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kate_top5_prompt3 = lambda s:kate_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars[f'kate_{p}_{_}'][s], a) \\\n",
        "        for _ in range(5) for p, a in [('pos', 'True'), ('neg', 'False')]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_top5_prompt3(0))\n",
        "\n",
        "# await chatgpt_loop(kate_top5_prompt3, max_tokens=2, filename_suffix='kate_top5_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "Y0P7skxX82GP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "least to most"
      ],
      "metadata": {
        "id": "CeICeo61iiEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = chatgpt\n",
        "run = 1\n",
        "# need to control three variables: question_lists_tq, exemplars_tq, assertion_tq\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars_tq[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt + exemplar_prompt(e, question_lists_tq).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion_tq = tst_data_df['assertion_tq'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion_tq, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion_tq, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists_tq[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists_tq[e][2].format(assertion_tq)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_gpt3.5-turbo_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "uVrNcYe0RmOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = chatgpt\n",
        "run = 2\n",
        "# need to control three variables: question_lists_tq, exemplars, assertion\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt + exemplar_prompt(e, question_lists_tq).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion = tst_data_df['assertion'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists_tq[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists_tq[e][2].format(assertion)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_gpt3.5-turbo_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "zddnxa9RRnVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = chatgpt\n",
        "run = 3\n",
        "# need to control three variables: question_lists, exemplars, instance\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt + exemplar_prompt(e, question_lists).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion = tst_data_df['assertion'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists[e][2].format(assertion)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_gpt3.5-turbo_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "o_s7MenNijVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "least to most no exemplar"
      ],
      "metadata": {
        "id": "T3SvOVLrkuWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = chatgpt\n",
        "run = '1_no_exemplar'\n",
        "# need to control three variables: question_lists_tq, exemplars_tq, assertion_tq\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars_tq[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt #+ exemplar_prompt(e, question_lists_tq).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion_tq = tst_data_df['assertion_tq'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion_tq, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion_tq, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists_tq[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists_tq[e][2].format(assertion_tq)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_gpt3.5-turbo_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "BLYQweFJkuWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = chatgpt\n",
        "run = '2_no_exemplar'\n",
        "# need to control three variables: question_lists_tq, exemplars, assertion\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt #+ exemplar_prompt(e, question_lists_tq).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion = tst_data_df['assertion'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists_tq[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists_tq[e][2].format(assertion)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_gpt3.5-turbo_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "PTze7WSKkuWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = chatgpt\n",
        "run = '3_no_exemplar'\n",
        "# need to control three variables: question_lists, exemplars, instance\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt #+ exemplar_prompt(e, question_lists).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion = tst_data_df['assertion'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists[e][2].format(assertion)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_gpt3.5-turbo_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_gpt3.5-turbo_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "7uWSOUiMkuWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "random exemplar"
      ],
      "metadata": {
        "id": "oI5Rl2nR-1cq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_5shot_prompt1 = lambda s:judge_prompt_0.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_random_exemplars[f'{_}_tq'][s], 'Yes' if tst_random_exemplars[f'{_}_tq_label'][s] else 'No') for _ in range(5)] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion_tq'][s], '')]\n",
        "))\n",
        "print(random_5shot_prompt1(0))\n",
        "\n",
        "await chatgpt_loop(random_5shot_prompt1, max_tokens=2, filename_suffix='random_5shot_prompt1', resume_at_step=0)"
      ],
      "metadata": {
        "id": "dZnxuZIw-48A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_5shot_prompt2 = lambda s:judge_prompt_0.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_random_exemplars[f'{_}'][s], 'Yes' if tst_random_exemplars[f'{_}_label'][s] else 'No') for _ in range(5)] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(random_5shot_prompt2(0))\n",
        "\n",
        "await chatgpt_loop(random_5shot_prompt2, max_tokens=2, filename_suffix='random_5shot_prompt2', resume_at_step=0)"
      ],
      "metadata": {
        "id": "KMZBs1ZrCAxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_5shot_prompt3 = lambda s:judge_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_random_exemplars[f'{_}'][s], 'True' if tst_random_exemplars[f'{_}_label'][s] else 'False') for _ in range(5, 10)] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(random_5shot_prompt3(0))\n",
        "\n",
        "# await chatgpt_loop(random_5shot_prompt3, max_tokens=2, filename_suffix='random_5shot_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "vkP5bEqtCCFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kate top 5 (regardless label, regardless relation)\n",
        "\n",
        "more similar to tst instance, the closer to the instance we place it. already checked https://pytorch.org/docs/stable/generated/torch.topk.html"
      ],
      "metadata": {
        "id": "rGCuvaocDXjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kate_5shot_prompt1 = lambda s:judge_prompt_0.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars_tq[f'{_}_tq'][s], 'Yes' if tst_kate_exemplars_tq[f'{_}_tq_label'][s] else 'No') for _ in [4,3,2,1,0]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion_tq'][s], '')]\n",
        "))\n",
        "print(kate_5shot_prompt1(0))\n",
        "\n",
        "# await chatgpt_loop(kate_5shot_prompt1, max_tokens=2, filename_suffix='kate_5shot_prompt1', resume_at_step=0)"
      ],
      "metadata": {
        "id": "BBzKOG2XDdLv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f9b3b53-a056-4cb2-f624-2eba3e79e640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer whether the following statement is plausible. Answer with only Yes or No:\n",
            "Statement: If PersonX push PersonY back, as a result, PersonY or others will, PeopleX step back from PersonX\n",
            "Answer: Yes\n",
            "\n",
            "Statement: If PersonX regain PersonY 's composure, can be hindered by, PersonY be disown personx\n",
            "Answer: Yes\n",
            "\n",
            "Statement: If PersonX be nowhere, can be hindered by, PersonX friend will not keep PersonY\n",
            "Answer: Yes\n",
            "\n",
            "Statement: If PersonX chase PersonZ away, as a result, PersonX will, PersonY lose friend\n",
            "Answer: Yes\n",
            "\n",
            "Statement: If PersonX wave PersonY away, as a result, PersonX will, PersonY roll PersonZ eye\n",
            "Answer: Yes\n",
            "\n",
            "Statement: If PersonX argue with PersonY, as a result, PersonY or others will, PersonY be not a PersonZ\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kate_5shot_prompt2 = lambda s:judge_prompt_0.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars[f'{_}'][s], 'Yes' if tst_kate_exemplars[f'{_}_label'][s] else 'No') for _ in [4,3,2,1,0]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_5shot_prompt2(0))\n",
        "\n",
        "# await chatgpt_loop(kate_5shot_prompt2, max_tokens=2, filename_suffix='kate_5shot_prompt2', resume_at_step=0)"
      ],
      "metadata": {
        "id": "QTYu2MOAEkP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b758f4b-6cf1-43c7-c18b-cd7970331aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer whether the following statement is plausible. Answer with only Yes or No:\n",
            "Statement: PersonX stay away from PersonY, thus as an result, PersonX call out to PersonX\n",
            "Answer: No\n",
            "\n",
            "Statement: PersonX help the PersonY, thus as an result, PersonX be rebuff by PersonY\n",
            "Answer: Yes\n",
            "\n",
            "Statement: PersonX turn down that, thus it can be seen about PersonX's attribute that PersonX get PersonY into trouble\n",
            "Answer: No\n",
            "\n",
            "Statement: PersonX be real, thus as an result, PersonY argue PersonZ about it\n",
            "Answer: Yes\n",
            "\n",
            "Statement: PersonX challenge PersonZ 's friend, thus, PersonY want PersonY not let\n",
            "Answer: Yes\n",
            "\n",
            "Statement: PersonX argue with PersonY, thus as an result, PersonY be not a PersonZ\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kate_5shot_prompt3 = lambda s:judge_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars[f'{_}'][s], 'True' if tst_kate_exemplars[f'{_}_label'][s] else 'False') for _ in [4,3,2,1,0]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_5shot_prompt3(0))\n",
        "\n",
        "# await chatgpt_loop(kate_5shot_prompt3, max_tokens=2, filename_suffix='kate_5shot_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "e9aaZtozEkjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb6456c-1931-49fb-a5bb-1676cc66ac7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Judge the following statement if it's likely to occur, only answer 'True' or 'False':\n",
            "Statement: PersonX stay away from PersonY, thus as an result, PersonX call out to PersonX\n",
            "Answer: False\n",
            "\n",
            "Statement: PersonX help the PersonY, thus as an result, PersonX be rebuff by PersonY\n",
            "Answer: True\n",
            "\n",
            "Statement: PersonX turn down that, thus it can be seen about PersonX's attribute that PersonX get PersonY into trouble\n",
            "Answer: False\n",
            "\n",
            "Statement: PersonX be real, thus as an result, PersonY argue PersonZ about it\n",
            "Answer: True\n",
            "\n",
            "Statement: PersonX challenge PersonZ 's friend, thus, PersonY want PersonY not let\n",
            "Answer: True\n",
            "\n",
            "Statement: PersonX argue with PersonY, thus as an result, PersonY be not a PersonZ\n",
            "Answer: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "kate top 5, exemplars from same relation (regardless label)"
      ],
      "metadata": {
        "id": "Y70F3FE3Fths"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kate_5shot_same_relation_prompt1 = lambda s:judge_prompt_0.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_same_relation_exemplars_tq[f'{_}_tq'][s], 'Yes' if tst_kate_same_relation_exemplars_tq[f'{_}_tq_label'][s] else 'No') for _ in [4,3,2,1,0]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion_tq'][s], '')]\n",
        "))\n",
        "print(kate_5shot_same_relation_prompt1(0))\n",
        "\n",
        "await chatgpt_loop(kate_5shot_same_relation_prompt1, max_tokens=2, filename_suffix='kate_5shot_same_relation_prompt1', resume_at_step=0)"
      ],
      "metadata": {
        "id": "13rZQKLSFvIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kate_5shot_same_relation_prompt2 = lambda s:judge_prompt_0.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_same_relation_exemplars[f'{_}'][s], 'Yes' if tst_kate_same_relation_exemplars[f'{_}_label'][s] else 'No') for _ in [4,3,2,1,0]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_5shot_same_relation_prompt2(0))\n",
        "\n",
        "await chatgpt_loop(kate_5shot_same_relation_prompt2, max_tokens=2, filename_suffix='kate_5shot_same_relation_prompt2', resume_at_step=0)"
      ],
      "metadata": {
        "id": "P_iAKOqwF_q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kate_5shot_same_relation_prompt3 = lambda s:judge_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_same_relation_exemplars[f'{_}'][s], 'True' if tst_kate_same_relation_exemplars[f'{_}_label'][s] else 'False') for _ in [4,3,2,1,0]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_5shot_same_relation_prompt3(0))\n",
        "\n",
        "await chatgpt_loop(kate_5shot_same_relation_prompt3, max_tokens=2, filename_suffix='kate_5shot_same_relation_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "LgL9u8h_GARb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czRER69OUfzo"
      },
      "source": [
        "### text-davinci-003\n",
        "\n",
        "'naive' was run on local runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "moe ~ system1"
      ],
      "metadata": {
        "id": "tYNpMLYH9wCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = tst_data_df[tst_data_df['relation'].isin('xReact,oReact,xAttr'.split(','))]\n",
        "temp_df.reset_index(inplace=True)\n",
        "total = (len(temp_df)-1)//bs+1\n",
        "temp_df\n",
        "\n",
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[moe_q1_prompt2.format(temp_df['tail'][s])\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(temp_df)))\n",
        "                ],\n",
        "                max_tokens=5,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/tst_davinci_moe_q1_prompt2.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "9j4_gpcyB9Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = tst_data_df[tst_data_df['relation'].isin('xIntent,xNeed'.split(','))]\n",
        "temp_df.reset_index(inplace=True)\n",
        "total = (len(temp_df)-1)//bs+1\n",
        "temp_df\n",
        "\n",
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[moe_q3_prompt1.format(temp_df['tail'][s], temp_df['head'][s])\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(temp_df)))\n",
        "                ],\n",
        "                max_tokens=5,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/tst_davinci_moe_q3_prompt1.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "imOWIPKpCM4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgniuLzyU2Iu"
      },
      "outputs": [],
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[moe_q1_prompt1.format(tst_data_df['tail'][s])\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                ],\n",
        "                max_tokens=5,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/tst_davinci_moe_q1_prompt1.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuIxDcbCbJMD"
      },
      "outputs": [],
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[moe_q3_prompt3.format(tst_data_df['tail'][s], tst_data_df['head'][s])\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                ],\n",
        "                max_tokens=5,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/tst_davinci_moe_q3_prompt3.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "zeroshotCoT"
      ],
      "metadata": {
        "id": "7gIH3GKeB7Px"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPXWscve1CRW"
      },
      "outputs": [],
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[zeroshotCoT_prompt_tq.format(tst_data_df['assertion'][s])\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                ],\n",
        "                max_tokens=50,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/tst_davinci_zeroshot_prompt1.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[zeroshotCoT_prompt1.format(tst_data_df['assertion_tq'][s])\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                ],\n",
        "                max_tokens=50,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/tst_davinci_zeroshot_prompt2.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "eF2b5DWx8Vs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[zeroshotCoT_prompt1.format(tst_data_df['assertion'][s])\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                ],\n",
        "                max_tokens=50,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/tst_davinci_zeroshot_prompt3.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "FuRbhnWy8WZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KATE"
      ],
      "metadata": {
        "id": "nAgpXHTqbp4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[kate_prompt_0.format('\\n\\n'.join([\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0_tq'][s], 'Yes'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0_tq'][s], 'No'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion_tq'][s], '')]))\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                ],\n",
        "                max_tokens=3,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/tst_davinci_kate_top1_prompt1.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "0WrWVQ9MgEZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[kate_prompt_0.format('\\n\\n'.join([\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0'][s], 'Yes'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0'][s], 'No'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]))\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                ],\n",
        "                max_tokens=3,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/tst_davinci_kate_top1_prompt2.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "28oZkZPLgEJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[kate_prompt_1.format('\\n\\n'.join([\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_pos_0'][s], 'True'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars['kate_neg_0'][s], 'False'),\n",
        "                            'Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]))\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))\n",
        "                ],\n",
        "                max_tokens=3,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/tst_davinci_kate_top1_prompt3.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "mHcoSslzbrCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kate_top5_prompt1 = lambda s:kate_prompt_0.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars[f'kate_{p}_{_}_tq'][s], a) \\\n",
        "        for _ in range(5) for p, a in [('pos', 'Yes'), ('neg', 'No')]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion_tq'][s], '')]\n",
        "))\n",
        "print(kate_top5_prompt1(0))\n",
        "\n",
        "davinci_loop(kate_top5_prompt1, max_tokens=4, filename_suffix='kate_top5_prompt1', resume_at_step=0)"
      ],
      "metadata": {
        "id": "msGFPjez1LqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kate_top5_prompt2 = lambda s:kate_prompt_0.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars[f'kate_{p}_{_}'][s], a) \\\n",
        "        for _ in range(5) for p, a in [('pos', 'Yes'), ('neg', 'No')]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_top5_prompt2(0))\n",
        "\n",
        "davinci_loop(kate_top5_prompt2, max_tokens=2, filename_suffix='kate_top5_prompt2', resume_at_step=0)"
      ],
      "metadata": {
        "id": "CoxtDGlX16kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kate_top5_prompt3 = lambda s:kate_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars[f'kate_{p}_{_}'][s], a) \\\n",
        "        for _ in range(5) for p, a in [('pos', 'True'), ('neg', 'False')]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_top5_prompt3(0))\n",
        "\n",
        "davinci_loop(kate_top5_prompt3, max_tokens=2, filename_suffix='kate_top5_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "ia2x1TJb1-6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "least2most"
      ],
      "metadata": {
        "id": "z50jmpeksu_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = davinci\n",
        "run = 1\n",
        "# need to control three variables: question_lists_tq, exemplars_tq, assertion_tq\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars_tq[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt + exemplar_prompt(e, question_lists_tq).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion_tq = tst_data_df['assertion_tq'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion_tq, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion_tq, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists_tq[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists_tq[e][2].format(assertion_tq)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_davinci_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "AiFwwFwFUrk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = davinci\n",
        "run = 2\n",
        "# need to control three variables: question_lists_tq, exemplars, assertion\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt + exemplar_prompt(e, question_lists_tq).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion = tst_data_df['assertion'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists_tq[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists_tq[e][2].format(assertion)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_davinci_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "GiPq3ipwUr4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = davinci\n",
        "run = 3\n",
        "# need to control three variables: question_lists, exemplars, assertion\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt + exemplar_prompt(e, question_lists).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion = tst_data_df['assertion'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists[e][2].format(assertion)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_davinci_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "f_M3t3d-oWs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "least2most no exemplar"
      ],
      "metadata": {
        "id": "WN5IxnUamo17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = davinci\n",
        "run = '1_no_exemplar'\n",
        "# need to control three variables: question_lists_tq, exemplars_tq, assertion_tq\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars_tq[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt #+ exemplar_prompt(e, question_lists_tq).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion_tq = tst_data_df['assertion_tq'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion_tq, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion_tq, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists_tq[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists_tq[e][2].format(assertion_tq)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_davinci_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "4bOG9FsImo17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = davinci\n",
        "run = '2_no_exemplar'\n",
        "# need to control three variables: question_lists_tq, exemplars, assertion\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt #+ exemplar_prompt(e, question_lists_tq).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion = tst_data_df['assertion'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists_tq[e][0].format(assertion, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists_tq[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists_tq[e][2].format(assertion)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_davinci_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "PwFRnl_Smo18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tst_data_df['prediction'] = 0\n",
        "func = davinci\n",
        "run = '3_no_exemplar'\n",
        "# need to control three variables: question_lists, exemplars, assertion\n",
        "\n",
        "for r in ['xReact', 'oReact', 'xAttr', 'xIntent', 'xNeed']:\n",
        "    e = 'typing' if r in ['xReact', 'oReact', 'xAttr'] else 'temporal'\n",
        "    arguments = [c for instance in exemplars[r] for c in instance]\n",
        "    constant_prompt = least2most_head_prompt #+ exemplar_prompt(e, question_lists).format(*arguments)\n",
        "    # print(constant_prompt); continue\n",
        "\n",
        "    for i in tst_data_df.index[tst_data_df['relation'] == r]:\n",
        "        tail = tst_data_df['tail'][i]\n",
        "        head = tst_data_df['head'][i]\n",
        "        assertion = tst_data_df['assertion'][i]\n",
        "        if e == 'typing':\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail)\n",
        "        else:\n",
        "            instance_prompt = question_lists[e][0].format(assertion, tail, head, tail, head)\n",
        "\n",
        "        # stage 1\n",
        "        answer = func(constant_prompt + instance_prompt, 5)\n",
        "        instance_prompt += answer + question_lists[e][1]\n",
        "\n",
        "        # stage 2\n",
        "        answer = func(constant_prompt + instance_prompt, 2)\n",
        "        instance_prompt += answer\n",
        "        if 'no' in answer.lower():\n",
        "            print(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "            output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "            output.close()\n",
        "            continue\n",
        "\n",
        "        # stage 3\n",
        "        instance_prompt += question_lists[e][2].format(assertion)\n",
        "        answer = func(constant_prompt + instance_prompt, 10)\n",
        "        instance_prompt += answer\n",
        "        if 'yes' in answer.lower():\n",
        "            tst_data_df['prediction'][i] = 1\n",
        "\n",
        "        print(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output = open(f'data/tst_davinci_least2most_prompt{run}_full.txt', 'a+')\n",
        "        output.write(f'--{i}\\n{instance_prompt}\\n')\n",
        "        output.close()\n",
        "\n",
        "tst_data_df['prediction'].to_csv(f'data/tst_davinci_least2most_prompt{run}.csv', index=False)"
      ],
      "metadata": {
        "id": "wgfJa7EEmo18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "re-run"
      ],
      "metadata": {
        "id": "hmA-NVUVMq64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(random_5shot_prompt1, max_tokens=2, filename_suffix='random_5shot_prompt1', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "vU_FUZ1RNZiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(random_5shot_prompt2, max_tokens=2, filename_suffix='random_5shot_prompt2', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "JcrH0LBpNZUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(random_5shot_prompt3, max_tokens=2, filename_suffix='random_5shot_prompt3', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "9oAZlmEbNZO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(kate_5shot_prompt1, max_tokens=2, filename_suffix='kate_5shot_prompt1', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "9jrH-y5hNZFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(kate_5shot_prompt2, max_tokens=2, filename_suffix='kate_5shot_prompt2', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "dnkEQJ5yNY9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(kate_5shot_prompt3, max_tokens=2, filename_suffix='kate_5shot_prompt3', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "Z505mOquNYzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(kate_5shot_same_relation_prompt1, max_tokens=2, filename_suffix='kate_5shot_same_relation_prompt1', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "vQp-O6ynNYpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(kate_5shot_same_relation_prompt2, max_tokens=2, filename_suffix='kate_5shot_same_relation_prompt2', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "YhAy5ChvNYcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(kate_5shot_same_relation_prompt3, max_tokens=2, filename_suffix='kate_5shot_same_relation_prompt3', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "-RYPvES5Msj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(naive_prompt1, max_tokens=6, filename_suffix='Jun17-naive_prompt1', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "ENOK4Mh-6dwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(naive_prompt2, max_tokens=6, filename_suffix='Jun17-naive_prompt2', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "ONV8omSYN9QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davinci_loop(naive_prompt3, max_tokens=6, filename_suffix='Jun17-naive_prompt3', resume_at_step=0)\n"
      ],
      "metadata": {
        "id": "gvnxYEhsOBBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wvlZpmhPl0Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalizability test ~ Different benchmark\n",
        "\n",
        "Run with prompt design 3 first, then if have time, run with other prompt designs"
      ],
      "metadata": {
        "id": "1gDHqBe_ErOw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19LmiwH5mE-u"
      },
      "outputs": [],
      "source": [
        "tst_data_df = pd.read_csv('data/atomic2020_w_negsample.csv')\n",
        "\n",
        "tst_kate_exemplars = pd.read_csv('data/ato_kate_exemplars_top5.csv')\n",
        "# tst_kate_exemplars_tq = pd.read_csv('data/tst_kate_exemplars_tq_top5.csv')\n",
        "tst_kate_same_relation_exemplars = pd.read_csv('data/ato_kate_same_relation_exemplars_top5.csv')\n",
        "# tst_kate_same_relation_exemplars_tq = pd.read_csv('data/tst_kate_same_relation_exemplars_tq_top5.csv')\n",
        "tst_random_exemplars = pd.read_csv('data/ato_random_5exemplars.csv')\n",
        "\n",
        "# print(tst_data_df.columns, len(tst_kate_exemplars))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for df in [tst_kate_exemplars,\n",
        "  # tst_kate_exemplars_tq,\n",
        "  tst_kate_same_relation_exemplars,\n",
        "  # tst_kate_same_relation_exemplars_tq,\n",
        "  tst_random_exemplars]:\n",
        "  print(df.columns)"
      ],
      "metadata": {
        "id": "9txCzdiXmE-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34a5a09-7351-436a-935f-4b1b2561e180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['0', '0_label', '1', '1_label', '2', '2_label', '3', '3_label', '4',\n",
            "       '4_label'],\n",
            "      dtype='object')\n",
            "Index(['0', '0_label', '1', '1_label', '2', '2_label', '3', '3_label', '4',\n",
            "       '4_label'],\n",
            "      dtype='object')\n",
            "Index(['0', '0_label', '1', '1_label', '2', '2_label', '3', '3_label', '4',\n",
            "       '4_label', '0_tq', '0_tq_label', '1_tq', '1_tq_label', '2_tq',\n",
            "       '2_tq_label', '3_tq', '3_tq_label', '4_tq', '4_tq_label', '5',\n",
            "       '5_label', '6', '6_label', '7', '7_label', '8', '8_label', '9',\n",
            "       '9_label'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHi3w5ngmE-v"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "# output = open('data/output.txt', 'a+')\n",
        "# output.close()\n",
        "# openai.api_key ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqhlfZBemE-w"
      },
      "outputs": [],
      "source": [
        "bs = 5 #@param\n",
        "total = (len(tst_data_df)-1)//bs+1\n",
        "timeout_succeed = 1 #@param\n",
        "timeout_failure = 10 #@param"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def chatgpt_loop_atomic(customize_prompt, max_tokens, filename_suffix, resume_at_step=0, end_at_step=total):\n",
        "    for i in range(resume_at_step, end_at_step):\n",
        "        while True:\n",
        "            try:\n",
        "                print(i, '/', total)\n",
        "                pred = await dispatch_openai_requests(\n",
        "                        messages_list = [[{\"role\": \"user\", \"content\": customize_prompt(s)}]\n",
        "                            for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))],\n",
        "                        max_tokens=max_tokens,\n",
        "                    )\n",
        "                temp = '\\n----------'.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "                print(temp)\n",
        "                output = open(f'data/atomic_gpt3.5-turbo_{filename_suffix}.txt', 'a+')\n",
        "                output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "                output.close()\n",
        "                predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "                time.sleep(timeout_succeed)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "VekiHpkGmE-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def davinci_loop_atomic(customize_prompt, max_tokens, filename_suffix, resume_at_step=0, end_at_step=total):\n",
        "    for i in range(resume_at_step, end_at_step):\n",
        "        while True:\n",
        "            try:\n",
        "                print(i, '/', total)\n",
        "                p = openai.Completion.create(\n",
        "                    model=\"gpt-3.5-turbo-instruct\",\n",
        "                    prompt=[customize_prompt(s)\n",
        "                        for s in range(i*bs, min((i+1)*bs, len(tst_data_df)))],\n",
        "                    max_tokens=max_tokens,\n",
        "                    temperature=0\n",
        "                )\n",
        "                temp = '\\n----------'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "                print(temp)\n",
        "                output = open(f'data/atomic_davinci_{filename_suffix}.txt', 'a+')\n",
        "                output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "                output.close()\n",
        "                predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "                time.sleep(timeout_succeed)\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "                time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "2jGD3x44mE-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cot_examplars_3 = \"\"\"Q: Judge the following statement if it's likely to occur: After PersonX find PersonY, PersonX wake up on ground.\n",
        "A: Let's think step by step. If PersonX just wake up on the ground, that means PersonX was sleeping before that. Thus, PersonX cannot be conscious to find another person. Thus, the statement is not likely to occur.\n",
        "\n",
        "Q: Judge the following statement if it's likely to occur: PeopleX deserve happiness, thus as a result on PersonX's emotion, PersonX reach out to PeopleX.\n",
        "A: Let's think step by step. The events 'PeopleX deserve happiness' and 'PersonX reach out to PeopleX' are likely irrevelant. Also, the clause 'PersonX reach out to PeopleX' does not describe from PersonX's emotion. Thus, the statement is not likely to occur.\n",
        "\n",
        "Q: Judge the following statement if it's likely to occur: The event PersonX have a sheet will not happen unless PersonX meet PersonY requirement.\n",
        "A: Let's think step by step. If PersonX doesn't meet PersonY requirement, PersonX likely doesn't get the reward from PersonY. However, in this case, it's not clear whether 'a sheet' refer to PersonY's reward or not. Thus, the statement is not likely to occur.\n",
        "\n",
        "Q: Judge the following statement if it's likely to occur: PersonX occupy PersonY position, thus, PersonX want PersonY want to aid in position.\n",
        "A: Let's think step by step. When PersonX occupy PersonY position, it means PersonY already worked at this position and has experience to do the job. Therefore, it's likely that PersonX want PersonY to aid PerosonX when PersonX is in that job position. Thus, the statement is likely to occur.\n",
        "\n",
        "Q: Judge the following statement if it's likely to occur: PersonX see that, thus as an result, PersonX want a pet.\n",
        "A: Let's think step by step. In this context, we can refer the word 'that' as some activity where people play with their pet. Therefore, it stimulates PersonX's desire to have a pet. Thus, the statement is likely to occur.\n",
        "\n",
        "Q: Judge the following statement if it's likely to occur: {}.\n",
        "A: \"\"\""
      ],
      "metadata": {
        "id": "1Cz1r6Z3PPgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gpt3.5-turbo"
      ],
      "metadata": {
        "id": "RDRaPUR4s6Zo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "naive"
      ],
      "metadata": {
        "id": "LfUxBfgcnw_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "judge_prompt_1 = \"Judge the following statement if it's likely to occur, only answer 'True' or 'False':\\n{}\"\n",
        "naive_prompt3 = lambda s:judge_prompt_1.format(tst_data_df['assertion'][s])\n",
        "print(naive_prompt3(0))\n",
        "\n",
        "await chatgpt_loop_atomic(naive_prompt3, max_tokens=2, filename_suffix='naive_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "vmKG8VpwEuFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "typing constraint"
      ],
      "metadata": {
        "id": "8WDsciCc6yL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_q1 = tst_data_df['relation'].isin(['xReact', 'oReact', 'xAttr'])\n",
        "temp_df = tst_data_df[idx_q1]\n",
        "temp_df.reset_index(inplace=True)\n",
        "total = (len(temp_df)-1)//bs+1\n",
        "# temp_df"
      ],
      "metadata": {
        "id": "zvjCBiwt5SJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 3\n",
        "prompt = moe_q1_prompt1.format(temp_df['tail'][i])\n",
        "print(prompt)\n",
        "chatgpt(prompt, max_tokens=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "Kil7SqP95ThJ",
        "outputId": "d5e0421a-6e47-4b84-d6fc-dc2d985d3612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause 'PersonX feel more rested for the next day' express. Answer the choice only.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3. mental'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run MoE first\n",
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q1_prompt1.format(s)}] for s in temp_df['tail'][i*bs:(i+1)*bs]],\n",
        "                    max_tokens=4,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/atomic_gpt3.5-turbo_moe_q1_prompt1.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "HUoBFyQcVK2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "temporal constraint"
      ],
      "metadata": {
        "id": "e9lp3_XE6utI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_q3 = tst_data_df['relation'].isin(['xIntent', 'xNeed'])\n",
        "temp_df = tst_data_df[idx_q3]\n",
        "temp_df.reset_index(inplace=True)\n",
        "total = (len(temp_df)-1)//bs+1\n",
        "# temp_df"
      ],
      "metadata": {
        "id": "McPNvjPc2bjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "prompt = moe_q3_prompt3.format(temp_df['tail'][i], temp_df['head'][i])\n",
        "print(prompt)\n",
        "chatgpt(prompt, max_tokens=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "kIgoBlYu0NvM",
        "outputId": "085dfc79-a79a-4f50-f6d8-2027db722476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which one of the following two statements is more plausible:\n",
            "0. PersonX swim in the water before PersonX pulls ___ out of the water\n",
            "1. PersonX swim in the water after PersonX pulls ___ out of the water\n",
            "Answer 0 or 1 only.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(b, '/', total)\n",
        "            pred = await dispatch_openai_requests(\n",
        "                    messages_list = [[{\"role\": \"user\", \"content\": moe_q3_prompt3.format(temp_df['tail'][i], temp_df['head'][i])}] \\\n",
        "                        for i in range(b*bs, min((b+1)*bs, len(temp_df)))],\n",
        "                    max_tokens=4,\n",
        "                )\n",
        "            temp = ', '.join([x['choices'][0]['message']['content'] for x in pred])\n",
        "            print(temp)\n",
        "            output = open('data/atomic_gpt3.5-turbo_moe_q3_prompt3.txt', 'a+')\n",
        "            output.write(f'{b}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['choices'][0]['message']['content'] for x in pred])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)\n"
      ],
      "metadata": {
        "id": "fIFFd-soyy3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "zeroshotCoT"
      ],
      "metadata": {
        "id": "kvQX59TxoDEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zeroshot_prompt3 = lambda s:zeroshotCoT_prompt1.format(tst_data_df['assertion'][s])\n",
        "print(zeroshot_prompt3(0))\n",
        "\n",
        "await chatgpt_loop_atomic(zeroshot_prompt3, max_tokens=100, filename_suffix='zeroshot_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "ati6AQd2oFnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "random exemplar"
      ],
      "metadata": {
        "id": "EIwErg4l-zq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_5shot_prompt3 = lambda s:judge_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_random_exemplars[f'{_}'][s], 'True' if tst_random_exemplars[f'{_}_label'][s] else 'False') for _ in range(5, 10)] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(random_5shot_prompt3(0))\n",
        "\n",
        "await chatgpt_loop_atomic(random_5shot_prompt3, max_tokens=2, filename_suffix='random_5shot_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "b0DepwnJ-zq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kate top 5 (regardless label, regardless relation)\n",
        "\n",
        "more similar to tst instance, the closer to the instance we place it. already checked https://pytorch.org/docs/stable/generated/torch.topk.html"
      ],
      "metadata": {
        "id": "DuH2Qg9g-zq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kate_5shot_prompt3 = lambda s:judge_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars[f'{_}'][s], 'True' if tst_kate_exemplars[f'{_}_label'][s] else 'False') for _ in [4,3,2,1,0]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_5shot_prompt3(0))\n",
        "\n",
        "await chatgpt_loop_atomic(kate_5shot_prompt3, max_tokens=2, filename_suffix='kate_5shot_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "1T1DjyDM-zq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kate top 5, exemplars from same relation (regardless label)"
      ],
      "metadata": {
        "id": "1wBiWraY-zq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kate_5shot_same_relation_prompt3 = lambda s:judge_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_same_relation_exemplars[f'{_}'][s], 'True' if tst_kate_same_relation_exemplars[f'{_}_label'][s] else 'False') for _ in [4,3,2,1,0]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_5shot_same_relation_prompt3(0))\n",
        "\n",
        "await chatgpt_loop_atomic(kate_5shot_same_relation_prompt3, max_tokens=2, filename_suffix='kate_5shot_same_relation_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "NiSwIU-2-zq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomcot_prompt3 = lambda s:cot_examplars_3.format(tst_data_df['assertion'][s])\n",
        "print(randomcot_prompt3(0))\n",
        "\n",
        "await chatgpt_loop_atomic(randomcot_prompt3, max_tokens=80, filename_suffix='randomcot_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "8b5zpJOBNYrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### text-davinci-003"
      ],
      "metadata": {
        "id": "yGT0j1g3s-8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_prompt3 = lambda s:judge_prompt_1.format(tst_data_df['assertion'][s])\n",
        "print(naive_prompt3(0))\n",
        "\n",
        "davinci_loop_atomic(naive_prompt3, max_tokens=10, filename_suffix='naive_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "SEUv-mCZU46o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_prompt2 = lambda s:judge_prompt_0.format(tst_data_df['assertion'][s])\n",
        "print(naive_prompt2(0))\n",
        "\n",
        "davinci_loop_atomic(naive_prompt2, max_tokens=10, filename_suffix='naive_prompt2', resume_at_step=3)"
      ],
      "metadata": {
        "id": "JVPe1kA8tAsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "typing constraint"
      ],
      "metadata": {
        "id": "-RPJna3Fmkuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = tst_data_df[tst_data_df['relation'].isin('xReact,oReact,xAttr'.split(','))]\n",
        "temp_df.reset_index(inplace=True)\n",
        "total = (len(temp_df)-1)//bs+1\n",
        "temp_df"
      ],
      "metadata": {
        "id": "LppvYoJamsZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "prompt = moe_q1_prompt1.format(temp_df['tail'][i])\n",
        "print(prompt)\n",
        "davinci(prompt, max_tokens=3)"
      ],
      "metadata": {
        "id": "5Gob7-9mmqIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[moe_q1_prompt1.format(temp_df['tail'][s])\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(temp_df)))\n",
        "                ],\n",
        "                max_tokens=5,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n----------'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/atomic_davinci_moe_q1_prompt1.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "eEoce8LslAOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "temporal constraint"
      ],
      "metadata": {
        "id": "5VJK7O13mmSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_df = tst_data_df[tst_data_df['relation'].isin('xIntent,xNeed'.split(','))]\n",
        "temp_df.reset_index(inplace=True)\n",
        "total = (len(temp_df)-1)//bs+1\n",
        "temp_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "S7rvBISim8sL",
        "outputId": "1774be32-9322-438b-a382-6cfbeb7266e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     index                                  head relation  \\\n",
              "0        2    PersonX pulls ___ out of the water    xNeed   \n",
              "1        3           PersonX makes PersonY smile  xIntent   \n",
              "2        7     PersonX does n't want to waste it  xIntent   \n",
              "3        8  PersonX evaluates the patient 's ___  xIntent   \n",
              "4       10               PersonX throws the ball  xIntent   \n",
              "..     ...                                   ...      ...   \n",
              "559    992               PersonX watches PersonY    xNeed   \n",
              "560    994          PersonX represents every ___    xNeed   \n",
              "561    995         PersonX finds ___ to describe  xIntent   \n",
              "562    997              PersonX is making a cake    xNeed   \n",
              "563    999    PersonX protects PersonY from harm  xIntent   \n",
              "\n",
              "                                              tail  label  modification  \\\n",
              "0                        PersonX swim in the water      1             0   \n",
              "1                               PersonX feel loved      0             2   \n",
              "2                            PersonX make a budget      0             2   \n",
              "3     PersonX intended to take care of the patient      1             0   \n",
              "4                   PersonX intended to play catch      1             0   \n",
              "..                                             ...    ...           ...   \n",
              "559                            PersonX be watchful      0             1   \n",
              "560                          PersonX be well known      1             0   \n",
              "561                      PersonX feel less worried      0             2   \n",
              "562                             PersonX feel happy      0             2   \n",
              "563  PersonX intended to keep him safe from danger      1             0   \n",
              "\n",
              "                                             assertion  \n",
              "0    The event PersonX pulls ___ out of the water w...  \n",
              "1    PersonX makes PersonY smile, thus it can be se...  \n",
              "2    PersonX does n't want to waste it, thus it can...  \n",
              "3    PersonX evaluates the patient 's ___, thus it ...  \n",
              "4    PersonX throws the ball, thus it can be seen a...  \n",
              "..                                                 ...  \n",
              "559  The event PersonX watches PersonY will not hap...  \n",
              "560  The event PersonX represents every ___ will no...  \n",
              "561  PersonX finds ___ to describe, thus it can be ...  \n",
              "562  The event PersonX is making a cake will not ha...  \n",
              "563  PersonX protects PersonY from harm, thus it ca...  \n",
              "\n",
              "[564 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d3759b4-f0b4-4fe3-b09a-70b99ab97dff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>head</th>\n",
              "      <th>relation</th>\n",
              "      <th>tail</th>\n",
              "      <th>label</th>\n",
              "      <th>modification</th>\n",
              "      <th>assertion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>PersonX pulls ___ out of the water</td>\n",
              "      <td>xNeed</td>\n",
              "      <td>PersonX swim in the water</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The event PersonX pulls ___ out of the water w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>PersonX makes PersonY smile</td>\n",
              "      <td>xIntent</td>\n",
              "      <td>PersonX feel loved</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>PersonX makes PersonY smile, thus it can be se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>PersonX does n't want to waste it</td>\n",
              "      <td>xIntent</td>\n",
              "      <td>PersonX make a budget</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>PersonX does n't want to waste it, thus it can...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>PersonX evaluates the patient 's ___</td>\n",
              "      <td>xIntent</td>\n",
              "      <td>PersonX intended to take care of the patient</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PersonX evaluates the patient 's ___, thus it ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>PersonX throws the ball</td>\n",
              "      <td>xIntent</td>\n",
              "      <td>PersonX intended to play catch</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PersonX throws the ball, thus it can be seen a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559</th>\n",
              "      <td>992</td>\n",
              "      <td>PersonX watches PersonY</td>\n",
              "      <td>xNeed</td>\n",
              "      <td>PersonX be watchful</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>The event PersonX watches PersonY will not hap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>560</th>\n",
              "      <td>994</td>\n",
              "      <td>PersonX represents every ___</td>\n",
              "      <td>xNeed</td>\n",
              "      <td>PersonX be well known</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>The event PersonX represents every ___ will no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>561</th>\n",
              "      <td>995</td>\n",
              "      <td>PersonX finds ___ to describe</td>\n",
              "      <td>xIntent</td>\n",
              "      <td>PersonX feel less worried</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>PersonX finds ___ to describe, thus it can be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562</th>\n",
              "      <td>997</td>\n",
              "      <td>PersonX is making a cake</td>\n",
              "      <td>xNeed</td>\n",
              "      <td>PersonX feel happy</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>The event PersonX is making a cake will not ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>563</th>\n",
              "      <td>999</td>\n",
              "      <td>PersonX protects PersonY from harm</td>\n",
              "      <td>xIntent</td>\n",
              "      <td>PersonX intended to keep him safe from danger</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PersonX protects PersonY from harm, thus it ca...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>564 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d3759b4-f0b4-4fe3-b09a-70b99ab97dff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d3759b4-f0b4-4fe3-b09a-70b99ab97dff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d3759b4-f0b4-4fe3-b09a-70b99ab97dff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-78f93410-75b4-4b81-a59a-a74adf4481ac\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-78f93410-75b4-4b81-a59a-a74adf4481ac')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-78f93410-75b4-4b81-a59a-a74adf4481ac button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 2\n",
        "prompt = moe_q3_prompt3.format(temp_df['tail'][i], temp_df['head'][i])\n",
        "print(prompt)\n",
        "davinci(prompt, max_tokens=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "mdrtEW68m9ds",
        "outputId": "7e164c0b-5614-47ef-d3ad-b10d50a5113d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which one of the following two statements is more plausible:\n",
            "0. PersonX make a budget before PersonX does n't want to waste it\n",
            "1. PersonX make a budget after PersonX does n't want to waste it\n",
            "Answer 0 or 1 only.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, total):\n",
        "    while True:\n",
        "        try:\n",
        "            print(i, '/', total)\n",
        "            p = openai.Completion.create(\n",
        "                model=\"text-davinci-003\",\n",
        "                prompt=[moe_q3_prompt3.format(temp_df['tail'][s], temp_df['head'][s])\n",
        "                    for s in range(i*bs, min((i+1)*bs, len(temp_df)))\n",
        "                ],\n",
        "                max_tokens=5,\n",
        "                temperature=0\n",
        "            )\n",
        "            temp = '\\n----------'.join([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            print(temp)\n",
        "            output = open('data/atomic_davinci_moe_q3_prompt3.txt', 'a+')\n",
        "            output.write(f'{i}/{total}\\n{temp}\\n')\n",
        "            output.close()\n",
        "            predictions.extend([x['text'].replace('\\n','--') for x in p['choices']])\n",
        "            time.sleep(timeout_succeed)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(timeout_failure)"
      ],
      "metadata": {
        "id": "_KwmGquXlAgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "other baselines"
      ],
      "metadata": {
        "id": "TeRIa9Tfn8Oe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zeroshot_prompt3 = lambda s:zeroshotCoT_prompt1.format(tst_data_df['assertion'][s])\n",
        "for i in range(1):\n",
        "  print(zeroshot_prompt3(i))\n",
        "\n",
        "davinci_loop_atomic(zeroshot_prompt3, max_tokens=100, filename_suffix='zeroshot_prompt3', resume_at_step=1, end_at_step=200)"
      ],
      "metadata": {
        "id": "HLMvcADW8iKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zeroshot_prompt3 = lambda s:zeroshotCoT_prompt1.format(tst_data_df['assertion'][s])\n",
        "for i in range(1):\n",
        "  print(zeroshot_prompt3(i))\n",
        "\n",
        "davinci(zeroshot_prompt3(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "wOAa88GqXfx0",
        "outputId": "3d4f17dd-b57a-4709-ca17-01a745a1e5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Judge the statement 'PersonX pries open, thus it can be seen about PersonX's attribute that PersonX tools' if it's likely to occur. Let's think step by step, then conclude by answering 'True' or 'False'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'False'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, 5):\n",
        "  print(zeroshot_prompt3(i))\n",
        "  print(davinci(zeroshot_prompt3(i), max_tokens=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FX1ZP1tYGKE",
        "outputId": "a48140f2-dde6-4071-872d-2677accfca30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Judge the statement 'PersonX sees a snake, thus it can be seen about PersonX's attribute that PersonX be frightened' if it's likely to occur. Let's think step by step, then conclude by answering 'True' or 'False'.\n",
            "True\n",
            "Judge the statement 'The event PersonX pulls ___ out of the water will not happen unless PersonX swim in the water' if it's likely to occur. Let's think step by step, then conclude by answering 'True' or 'False'.\n",
            "True\n",
            "Judge the statement 'PersonX makes PersonY smile, thus it can be seen about PersonX's intention that PersonX feel loved' if it's likely to occur. Let's think step by step, then conclude by answering 'True' or 'False'.\n",
            "Step 1: PersonX makes PersonY smile.\n",
            "- This statement is likely to occur, as it is a simple action that can easily be done by PersonX.\n",
            "\n",
            "Step 2: It can be seen about PersonX's intention that PersonX feels loved.\n",
            "- This statement is not necessarily true. While making someone smile can be a sign of affection and love, it does not necessarily mean that PersonX's intention is for PersonY to feel loved. PersonX could simply be trying to\n",
            "Judge the statement 'PersonX can play with PersonY, thus as a result on PersonX's emotion, PersonX decide to play a game' if it's likely to occur. Let's think step by step, then conclude by answering 'True' or 'False'.\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_5shot_prompt3 = lambda s:judge_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_random_exemplars[f'{_}'][s], 'True' if tst_random_exemplars[f'{_}_label'][s] else 'False') for _ in range(5, 10)] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(random_5shot_prompt3(0))\n",
        "\n",
        "davinci_loop_atomic(random_5shot_prompt3, max_tokens=2, filename_suffix='random_5shot_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "sQlOnjzjNh0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kate_5shot_prompt3 = lambda s:judge_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_exemplars[f'{_}'][s], 'True' if tst_kate_exemplars[f'{_}_label'][s] else 'False') for _ in [4,3,2,1,0]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_5shot_prompt3(0))\n",
        "\n",
        "davinci_loop_atomic(kate_5shot_prompt3, max_tokens=2, filename_suffix='kate_5shot_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "u4c9Gnl3NuVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kate_5shot_same_relation_prompt3 = lambda s:judge_prompt_1.format('\\n\\n'.join(\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_kate_same_relation_exemplars[f'{_}'][s], 'True' if tst_kate_same_relation_exemplars[f'{_}_label'][s] else 'False') for _ in [4,3,2,1,0]] +\n",
        "    ['Statement: {}\\nAnswer: {}'.format(tst_data_df['assertion'][s], '')]\n",
        "))\n",
        "print(kate_5shot_same_relation_prompt3(0))\n",
        "\n",
        "davinci_loop_atomic(kate_5shot_same_relation_prompt3, max_tokens=2, filename_suffix='kate_5shot_same_relation_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "SamfiEEANy9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomcot_prompt3 = lambda s:cot_examplars_3.format(tst_data_df['assertion'][s])\n",
        "print(randomcot_prompt3(0))\n",
        "\n",
        "davinci_loop_atomic(randomcot_prompt3, max_tokens=80, filename_suffix='randomcot_prompt3', resume_at_step=0)"
      ],
      "metadata": {
        "id": "CZRXGqeHOCM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS1vYW7waeBK"
      },
      "source": [
        "## Summary of baselines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fout = open('data/output_kate_davinci_June9.txt', 'a+')\n",
        "fout.write('\\n'.join([str(x) for x in predictions]))\n",
        "fout.close()"
      ],
      "metadata": {
        "id": "rPPsvHQbxcVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# naive\n",
        "naive1 = \"Answer whether the following statement is plausible. Answer with only Yes or No:\\n{}\".format(tst_data_df['assertion_tq'])\n",
        "naive2 = \"Answer whether the following statement is plausible. Answer with only Yes or No:\\n{}\".format(tst_data_df['assertion'])\n",
        "naive3 = \"Judge the following statement if it's likely to occur, only answer 'True' or 'False':\\n{}\".format(tst_data_df['assertion'])"
      ],
      "metadata": {
        "id": "WBHUP2cr0uSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# naive + kate(-sr)"
      ],
      "metadata": {
        "id": "27KQTCR-1Le6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zeroshot CoT.\n",
        "zeroshotCoT_prompt_tq = \"Answer whether the statement '{}' is plausible. Let's think step by step, then conclude by answering 'True' or 'False'.\"\n",
        "zeroshotCoT_prompt1 = \"Judge the statement '{}' if it's likely to occur. Let's think step by step, then conclude by answering 'True' or 'False'.\" # if add only, it will strictly follow that command, only produce 'True' or 'False' w/o CoT\n",
        "\n",
        "# the combination zeroshotCoT_prompt_tq + assertion_tq is so baddd!\n",
        "zeroshot_prompt1 = zeroshotCoT_prompt_tq.format(assertion)\n",
        "zeroshot_prompt2 = zeroshotCoT_prompt1.format(assertion_tq)\n",
        "zeroshot_prompt3 = zeroshotCoT_prompt1.format(assertion)"
      ],
      "metadata": {
        "id": "3WMW0CMl1Lw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activeCoT\n",
        "# automateCoT"
      ],
      "metadata": {
        "id": "vPaaySHp1MJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# least2most. this setting will get the final answer directly (not as plug-and-play as system1)\n",
        "# please refer to the first two cells in 'Least to Most' subsection"
      ],
      "metadata": {
        "id": "T45cxfeE30Xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# system1\n",
        "# serve as an add-on. Only run once as deterministic.\n",
        "\n",
        "# typing\n",
        "moe_q1_prompt1 = \"Which aspect (among three options 1. event/activity, 2. persona, 3. mental state) of the subject does the clause '{}' express. Answer the choice only.\"\n",
        "\n",
        "# temporal\n",
        "moe_q3_prompt3 = \"\"\"Which one of the following two statements is more plausible:\n",
        "0. {0} before {1}\n",
        "1. {0} after {1}\n",
        "Answer 0 or 1 only.\"\"\" # tail [] head, 1 -> after, 0 -> before\n",
        "\n",
        "# ablation ~ different prompt choice for typing and temporal constraint\n",
        "moe_q1_prompt2 = \"Determine if clause '{}' expresses an event or activity of the subject. Answer 'Yes' or 'No' only.\"\n",
        "moe_q3_prompt1 = \"Judge if the event '{}' likely occurs after the event '{}'. Answer 'Yes' or 'No' only.\"    # pretty unstable\n",
        "\n",
        "# in the result table in Overleaf\n",
        "# (baselineX + moe) = avg(baselineX i + moe for i from 1 to 3)"
      ],
      "metadata": {
        "id": "x2ddP6nN5FkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activeCoT, pool of exemplar\n",
        "{\n",
        "    \"prompt\": [\n",
        "        {\n",
        "            \"question\": \"Q: Judge the following statement if it's likely to occur, only answer 'True' or 'False': 'PersonX learns to play the trumpet, thus, PersonY wants to applaud PersonX's talent.'\\nA:\",\n",
        "            \"rationale\": \"\",\n",
        "            \"pred_ans\": \"True\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Q: Judge the following statement if it's likely to occur, only answer 'True' or 'False': 'PersonX eat most of those, thus as a result on PersonX's emotion, PersonX feel hungry.'\\nA:\",\n",
        "            \"rationale\": \"\",\n",
        "            \"pred_ans\": \"False\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Q: Judge the following statement if it's likely to occur, only answer 'True' or 'False': 'PersonX do not feel well, thus as a result on PersonX's emotion, PersonX only make it through half.'\\nA:\",\n",
        "            \"rationale\": \"\",\n",
        "            \"pred_ans\": \"False\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Q: Judge the following statement if it's likely to occur, only answer 'True' or 'False': 'Before PersonX really empathize with PersonY, PersonX feel.'\\nA:\",\n",
        "            \"rationale\": \"\",\n",
        "            \"pred_ans\": \"False\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Q: Judge the following statement if it's likely to occur, only answer 'True' or 'False': 'PersonX have a traumatic incident, thus it can be seen about PersonX's attribute that PersonX be introvert'\\nA:\",\n",
        "            \"rationale\": \"\",\n",
        "            \"pred_ans\": \"False\"\n",
        "        }\n",
        "\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "wr03e88sb4Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgBzuFqywa54"
      },
      "source": [
        "## End"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}